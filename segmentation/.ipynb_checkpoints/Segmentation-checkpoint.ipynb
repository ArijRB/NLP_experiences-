{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tools import *\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "#import pyLDAvis.sklearn\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/ap/ap.txt'\n",
    "f = open(filename, 'r')\n",
    "data= f.read()\n",
    "soup = BeautifulSoup(\"<DOCS>\"+data+\"</DOCS>\", \"xml\")\n",
    "contents = soup.find_all('TEXT') \n",
    "print(len(contents))\n",
    "all_text = []\n",
    "for content in contents:\n",
    "    all_text.append(content.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = [e.replace(\"\\n\",\"\") for e in all_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-traitement des données : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on enleve les nombres et  les \"stopwords et  on a fixé min_df=0.001,max_df=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring terms in the vocabulary that have a document frequency < 0.001\n",
      "Ignoring terms in the vocabulary that have a document frequency > 0.3\n",
      "Removing numbers\n"
     ]
    }
   ],
   "source": [
    "vectorizer,X = build_vectorizer(all_text, stopwords=stopwords, b_rmnumbers=True,min_df=0.001,max_df=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arij/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=5, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Trouver les clusters à  la main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_zd = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_wz = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_wz_sorted = np.argsort(p_wz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 13662)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_wz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say' 'women' 'get' 'night' 'health' 'water' 'fire' 'died' 'show' 'world'\n",
      " 'life' 'center' 'city' 'day' 'home' 'family' 'hospital' 'like' 'children'\n",
      " 'old']\n",
      "['exchange' 'rates' 'cents' 'york' 'share' 'price' 'trading' 'higher'\n",
      " 'rose' 'trade' 'sales' 'dollar' 'oil' 'company' 'prices' 'stock' 'market'\n",
      " 'billion' 'million' 'percent']\n",
      "['think' 'support' 'going' 'budget' 'defense' 'democratic' 'national'\n",
      " 'states' 'bill' 'senate' 'administration' 'congress' 'committee'\n",
      " 'percent' 'state' 'reagan' 'campaign' 'dukakis' 'house' 'bush']\n",
      "['today' 'since' 'country' 'army' 'war' 'official' 'east' 'union' 'states'\n",
      " 'minister' 'west' 'gorbachev' 'officials' 'police' 'south' 'military'\n",
      " 'united' 'party' 'government' 'soviet']\n",
      "['law' 'prison' 'charges' 'government' 'office' 'trial' 'workers'\n",
      " 'company' 'city' 'million' 'attorney' 'judge' 'department' 'drug'\n",
      " 'officials' 'case' 'state' 'federal' 'police' 'court']\n"
     ]
    }
   ],
   "source": [
    "#Pour définir un sujet à chaque cluster\n",
    "for k in range(n):\n",
    "    print(voc[p_wz_sorted[k,13642:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/May/2019 11:48:25] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/May/2019 11:48:25] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/May/2019 11:48:25] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/May/2019 11:48:25] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    "visu = pyLDAvis.sklearn.prepare(lda, X, vectorizer)\n",
    "pyLDAvis.show(visu)\n",
    "#la résultat est en capture d'ecran \n",
    "#Un cluster est principalement défini par ses mots les plus saillants : ceux qui apparaissent beaucoup dans ce cluster mais peu ailleurs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inférence sur un nouveau document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = [\"Arij is sentenced to 10 years of prison due to heroin consumption. According to NBC, she is reportedly not a \\\n",
    "    consumer but a dealer.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = vectorizer.transform(sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11621487, 0.39956597, 0.01830722, 0.01837959, 0.44753235]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(x)\n",
    "#les resultats se correspondent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitemet d'un corpus étiqueté : Reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on vectorise les données sous la forme de BoW en fréquence en utilisant notre vectrozier .Puis, on applique l’algorithme LDA avec 90 clusters (le nombre de sujets originaux dans la base Reuters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring terms in the vocabulary that have a document frequency < 0.001\n",
      "Ignoring terms in the vocabulary that have a document frequency > 0.3\n",
      "Removing numbers\n"
     ]
    }
   ],
   "source": [
    "#lien ou jai trouvé comment j'i recupré les données\n",
    "#https://martin-thoma.com/nlp-reuters/\n",
    "labels = reuters.categories()\n",
    "mlb = MultiLabelBinarizer()\n",
    "docs = reuters.fileids()\n",
    "test = [d for d in docs if d.startswith('test/')]\n",
    "train = [d for d in docs if d.startswith('training/')]\n",
    "vectorizer,X = build_vectorizer(docs_['train'], stopwords=stopwords, b_rmnumbers=True,min_df=0.001,max_df=0.3)\n",
    "\n",
    "docs_ = {}\n",
    "docs_['train'] = [reuters.raw(doc_id) for doc_id in train]\n",
    "docs_['test'] = [reuters.raw(doc_id) for doc_id in test]\n",
    "xs = {'train': [], 'test': []}\n",
    "xs['train'] = vectorizer.fit_transform(docs_['train']).toarray()\n",
    "xs['test'] = vectorizer.transform(docs_['test']).toarray()\n",
    "ys = {'train': [], 'test': []}\n",
    "ys['train'] = mlb.fit_transform([reuters.categories(doc_id)\n",
    "                                     for doc_id in train])\n",
    "ys['test'] = mlb.transform([reuters.categories(doc_id)\n",
    "                                for doc_id in test])\n",
    "data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "            'x_test': xs['test'], 'y_test': ys['test'],\n",
    "            'labels': globals()[\"labels\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arij/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "/home/arij/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=50,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=1, n_topics=90,\n",
       "             perp_tol=0.1, random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data[\"x_train\"]\n",
    "n=90 #nombre des classes\n",
    "target = data[\"y_train\"]\n",
    "lda2 = LatentDirichletAllocation(max_iter=50,n_topics=n)\n",
    "lda2.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de pureté du premier cluster\n",
    "#lien qui explique le calcul de purete\n",
    "# https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html \n",
    "# ce lien est juste pour comprendre il sagit de quoi exactement car on a pas dans le cours\n",
    "#https://stackoverflow.com/questions/34047540/python-clustering-purity-metric/ \n",
    "# ce lien stackoverflow est mieux expliqué et donne les bouts de codes j'ai testé la version qu'on est la plus rapide\n",
    "def compute_purity(classe):    \n",
    "    pred_=[] #les predections affectees a la  classe\n",
    "    for k in range(len(X)):\n",
    "        pred = np.argmax(lda2.transform([X[k]])[0])# la classe predite pour ce document\n",
    "        if(pred==classe):\n",
    "            pred_.append(k)\n",
    "    true_=[] # les valeurs réélles qui existent deja dans le data les vraies valeurs\n",
    "    for p in pred_:\n",
    "        true_.append(np.argmax(data[\"y_train\"][p]))\n",
    "    purity=0\n",
    "    if len(true_)!=0 and len(pred_)!=0:# teste dans le cas ou on a pas attribué des elements a cette classe\n",
    "        contingency_matrix = metrics.cluster.contingency_matrix(true_, pred_)\n",
    "        purity=np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=0\n",
    "for i in range(1,91):\n",
    "    s+=compute_purity(i)\n",
    "purity_total= s/90\n",
    "purity_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
