{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 : Neural Embeddings, Text Classification, Text Generation\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **bag of word** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## Dataset\n",
    "https://github.com/cedias/practicalNLP/tree/master/dataset\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (only here for reference)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Loading data (same as in nlp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"dataset/json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train (or load) a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 12:00:59,363 : INFO : collecting all words and their counts\n",
      "2019-05-03 12:00:59,364 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-03 12:01:00,743 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n",
      "2019-05-03 12:01:02,398 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2019-05-03 12:01:03,147 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2019-05-03 12:01:03,152 : INFO : Loading a fresh vocabulary\n",
      "2019-05-03 12:01:04,000 : INFO : effective_min_count=5 retains 49345 unique words (17% of original 280617, drops 231272)\n",
      "2019-05-03 12:01:04,002 : INFO : effective_min_count=5 leaves 5517507 word corpus (94% of original 5844680, drops 327173)\n",
      "2019-05-03 12:01:04,533 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2019-05-03 12:01:04,567 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2019-05-03 12:01:04,568 : INFO : downsampling leaves estimated 4268608 word corpus (77.4% of prior 5517507)\n",
      "2019-05-03 12:01:04,956 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2019-05-03 12:01:04,957 : INFO : resetting layer weights\n",
      "2019-05-03 12:01:06,489 : INFO : training model with 3 workers on 49345 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-03 12:01:07,541 : INFO : EPOCH 1 - PROGRESS: at 1.89% examples, 84838 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:08,545 : INFO : EPOCH 1 - PROGRESS: at 4.24% examples, 92182 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:09,563 : INFO : EPOCH 1 - PROGRESS: at 6.49% examples, 94176 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:10,566 : INFO : EPOCH 1 - PROGRESS: at 8.68% examples, 93707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:11,659 : INFO : EPOCH 1 - PROGRESS: at 11.22% examples, 95023 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:12,714 : INFO : EPOCH 1 - PROGRESS: at 13.52% examples, 94691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:13,759 : INFO : EPOCH 1 - PROGRESS: at 18.27% examples, 108704 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:14,779 : INFO : EPOCH 1 - PROGRESS: at 22.48% examples, 116823 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:15,809 : INFO : EPOCH 1 - PROGRESS: at 26.76% examples, 123976 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:16,818 : INFO : EPOCH 1 - PROGRESS: at 31.12% examples, 129928 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:17,820 : INFO : EPOCH 1 - PROGRESS: at 35.15% examples, 134177 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:18,858 : INFO : EPOCH 1 - PROGRESS: at 39.81% examples, 138624 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:19,870 : INFO : EPOCH 1 - PROGRESS: at 44.34% examples, 142590 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:20,883 : INFO : EPOCH 1 - PROGRESS: at 48.51% examples, 145492 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:21,937 : INFO : EPOCH 1 - PROGRESS: at 53.11% examples, 148159 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:22,998 : INFO : EPOCH 1 - PROGRESS: at 57.88% examples, 150806 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:24,029 : INFO : EPOCH 1 - PROGRESS: at 62.42% examples, 153358 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:25,047 : INFO : EPOCH 1 - PROGRESS: at 67.20% examples, 155436 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:26,146 : INFO : EPOCH 1 - PROGRESS: at 71.96% examples, 156651 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:27,190 : INFO : EPOCH 1 - PROGRESS: at 76.49% examples, 158186 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:01:28,263 : INFO : EPOCH 1 - PROGRESS: at 81.22% examples, 159387 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:29,287 : INFO : EPOCH 1 - PROGRESS: at 85.65% examples, 160790 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:30,367 : INFO : EPOCH 1 - PROGRESS: at 90.06% examples, 161390 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:31,410 : INFO : EPOCH 1 - PROGRESS: at 94.68% examples, 162492 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:32,422 : INFO : EPOCH 1 - PROGRESS: at 99.11% examples, 163393 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:32,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 12:01:32,542 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 12:01:32,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 12:01:32,559 : INFO : EPOCH - 1 : training on 5844680 raw words (4269276 effective words) took 26.0s, 164020 effective words/s\n",
      "2019-05-03 12:01:33,563 : INFO : EPOCH 2 - PROGRESS: at 4.24% examples, 185364 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:34,566 : INFO : EPOCH 2 - PROGRESS: at 8.48% examples, 184969 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:35,574 : INFO : EPOCH 2 - PROGRESS: at 12.87% examples, 184814 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:36,624 : INFO : EPOCH 2 - PROGRESS: at 17.41% examples, 184525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:37,637 : INFO : EPOCH 2 - PROGRESS: at 21.95% examples, 185616 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:38,649 : INFO : EPOCH 2 - PROGRESS: at 26.46% examples, 186566 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:39,666 : INFO : EPOCH 2 - PROGRESS: at 30.79% examples, 186072 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:40,695 : INFO : EPOCH 2 - PROGRESS: at 34.82% examples, 184404 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:41,709 : INFO : EPOCH 2 - PROGRESS: at 39.48% examples, 185187 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:42,740 : INFO : EPOCH 2 - PROGRESS: at 43.84% examples, 184730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:43,903 : INFO : EPOCH 2 - PROGRESS: at 48.17% examples, 182809 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:44,918 : INFO : EPOCH 2 - PROGRESS: at 50.35% examples, 175403 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:46,012 : INFO : EPOCH 2 - PROGRESS: at 52.18% examples, 167043 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:47,105 : INFO : EPOCH 2 - PROGRESS: at 54.74% examples, 161799 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:48,204 : INFO : EPOCH 2 - PROGRESS: at 57.34% examples, 157330 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:49,259 : INFO : EPOCH 2 - PROGRESS: at 59.58% examples, 153019 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:50,259 : INFO : EPOCH 2 - PROGRESS: at 62.08% examples, 150799 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:51,264 : INFO : EPOCH 2 - PROGRESS: at 64.89% examples, 148842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:52,339 : INFO : EPOCH 2 - PROGRESS: at 67.70% examples, 146599 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:53,355 : INFO : EPOCH 2 - PROGRESS: at 69.84% examples, 143602 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:54,408 : INFO : EPOCH 2 - PROGRESS: at 72.65% examples, 141973 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:55,413 : INFO : EPOCH 2 - PROGRESS: at 75.30% examples, 140766 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:56,558 : INFO : EPOCH 2 - PROGRESS: at 78.47% examples, 139494 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:57,655 : INFO : EPOCH 2 - PROGRESS: at 81.07% examples, 137724 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:01:58,750 : INFO : EPOCH 2 - PROGRESS: at 83.86% examples, 136674 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:01:59,765 : INFO : EPOCH 2 - PROGRESS: at 86.26% examples, 135542 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:00,847 : INFO : EPOCH 2 - PROGRESS: at 89.56% examples, 135205 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:01,948 : INFO : EPOCH 2 - PROGRESS: at 92.02% examples, 133840 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:03,014 : INFO : EPOCH 2 - PROGRESS: at 94.68% examples, 132723 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:02:04,026 : INFO : EPOCH 2 - PROGRESS: at 96.76% examples, 131428 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:05,069 : INFO : EPOCH 2 - PROGRESS: at 99.63% examples, 130780 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-03 12:02:05,121 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 12:02:05,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 12:02:05,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 12:02:05,170 : INFO : EPOCH - 2 : training on 5844680 raw words (4268280 effective words) took 32.6s, 130892 effective words/s\n",
      "2019-05-03 12:02:06,236 : INFO : EPOCH 3 - PROGRESS: at 2.08% examples, 88862 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:07,327 : INFO : EPOCH 3 - PROGRESS: at 4.41% examples, 90450 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:08,359 : INFO : EPOCH 3 - PROGRESS: at 6.51% examples, 90306 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:09,567 : INFO : EPOCH 3 - PROGRESS: at 8.78% examples, 88112 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-03 12:02:10,688 : INFO : EPOCH 3 - PROGRESS: at 11.40% examples, 89718 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:11,723 : INFO : EPOCH 3 - PROGRESS: at 13.35% examples, 88569 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:12,815 : INFO : EPOCH 3 - PROGRESS: at 15.40% examples, 87225 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:13,867 : INFO : EPOCH 3 - PROGRESS: at 17.90% examples, 88966 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:14,892 : INFO : EPOCH 3 - PROGRESS: at 20.24% examples, 89831 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:02:16,030 : INFO : EPOCH 3 - PROGRESS: at 22.28% examples, 88261 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:17,079 : INFO : EPOCH 3 - PROGRESS: at 24.87% examples, 89594 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:18,179 : INFO : EPOCH 3 - PROGRESS: at 26.59% examples, 88057 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:19,225 : INFO : EPOCH 3 - PROGRESS: at 28.69% examples, 88143 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:20,307 : INFO : EPOCH 3 - PROGRESS: at 30.95% examples, 87974 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:21,359 : INFO : EPOCH 3 - PROGRESS: at 32.76% examples, 87072 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:22,403 : INFO : EPOCH 3 - PROGRESS: at 34.28% examples, 85953 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:23,527 : INFO : EPOCH 3 - PROGRESS: at 36.36% examples, 85382 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:24,632 : INFO : EPOCH 3 - PROGRESS: at 38.74% examples, 85681 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:25,665 : INFO : EPOCH 3 - PROGRESS: at 40.88% examples, 85582 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:02:26,744 : INFO : EPOCH 3 - PROGRESS: at 42.35% examples, 84285 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:27,797 : INFO : EPOCH 3 - PROGRESS: at 44.17% examples, 83853 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:28,845 : INFO : EPOCH 3 - PROGRESS: at 46.19% examples, 84062 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:29,970 : INFO : EPOCH 3 - PROGRESS: at 48.99% examples, 85154 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:31,043 : INFO : EPOCH 3 - PROGRESS: at 51.64% examples, 86099 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:32,189 : INFO : EPOCH 3 - PROGRESS: at 54.93% examples, 87458 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:33,226 : INFO : EPOCH 3 - PROGRESS: at 57.34% examples, 87814 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:34,310 : INFO : EPOCH 3 - PROGRESS: at 59.43% examples, 87523 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:35,315 : INFO : EPOCH 3 - PROGRESS: at 61.76% examples, 88138 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:36,367 : INFO : EPOCH 3 - PROGRESS: at 63.92% examples, 88159 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:37,430 : INFO : EPOCH 3 - PROGRESS: at 66.50% examples, 88388 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:38,509 : INFO : EPOCH 3 - PROGRESS: at 69.10% examples, 88777 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:39,530 : INFO : EPOCH 3 - PROGRESS: at 71.41% examples, 88869 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:40,591 : INFO : EPOCH 3 - PROGRESS: at 73.98% examples, 89262 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:41,716 : INFO : EPOCH 3 - PROGRESS: at 76.66% examples, 89683 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:42,798 : INFO : EPOCH 3 - PROGRESS: at 79.16% examples, 89802 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:43,888 : INFO : EPOCH 3 - PROGRESS: at 81.24% examples, 89524 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:02:44,904 : INFO : EPOCH 3 - PROGRESS: at 83.34% examples, 89614 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:45,931 : INFO : EPOCH 3 - PROGRESS: at 85.83% examples, 89996 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:46,934 : INFO : EPOCH 3 - PROGRESS: at 88.00% examples, 90090 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:48,032 : INFO : EPOCH 3 - PROGRESS: at 90.22% examples, 89967 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:02:49,063 : INFO : EPOCH 3 - PROGRESS: at 92.64% examples, 90322 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:50,112 : INFO : EPOCH 3 - PROGRESS: at 95.11% examples, 90468 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:51,119 : INFO : EPOCH 3 - PROGRESS: at 97.82% examples, 91003 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:51,994 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 12:02:52,013 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 12:02:52,131 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 90944 words/s, in_qsize 0, out_qsize 1\n",
      "2019-05-03 12:02:52,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 12:02:52,137 : INFO : EPOCH - 3 : training on 5844680 raw words (4268660 effective words) took 46.9s, 90932 effective words/s\n",
      "2019-05-03 12:02:53,168 : INFO : EPOCH 4 - PROGRESS: at 1.89% examples, 84324 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:54,271 : INFO : EPOCH 4 - PROGRESS: at 4.58% examples, 94328 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:55,333 : INFO : EPOCH 4 - PROGRESS: at 7.80% examples, 107617 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:56,496 : INFO : EPOCH 4 - PROGRESS: at 9.68% examples, 96937 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:57,746 : INFO : EPOCH 4 - PROGRESS: at 11.72% examples, 90638 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:02:58,868 : INFO : EPOCH 4 - PROGRESS: at 13.35% examples, 86067 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:02:59,938 : INFO : EPOCH 4 - PROGRESS: at 15.36% examples, 85328 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:00,971 : INFO : EPOCH 4 - PROGRESS: at 17.41% examples, 85014 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:02,146 : INFO : EPOCH 4 - PROGRESS: at 19.43% examples, 83608 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:03,159 : INFO : EPOCH 4 - PROGRESS: at 21.60% examples, 84320 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:04,185 : INFO : EPOCH 4 - PROGRESS: at 23.77% examples, 84857 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:05,224 : INFO : EPOCH 4 - PROGRESS: at 25.68% examples, 84161 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:03:06,306 : INFO : EPOCH 4 - PROGRESS: at 27.58% examples, 83829 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:07,495 : INFO : EPOCH 4 - PROGRESS: at 29.80% examples, 83403 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:08,569 : INFO : EPOCH 4 - PROGRESS: at 31.75% examples, 83145 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:03:09,748 : INFO : EPOCH 4 - PROGRESS: at 33.64% examples, 82440 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:10,799 : INFO : EPOCH 4 - PROGRESS: at 35.32% examples, 81631 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:11,875 : INFO : EPOCH 4 - PROGRESS: at 37.39% examples, 81543 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:12,915 : INFO : EPOCH 4 - PROGRESS: at 39.48% examples, 81597 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:13,963 : INFO : EPOCH 4 - PROGRESS: at 41.88% examples, 82280 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:14,976 : INFO : EPOCH 4 - PROGRESS: at 44.00% examples, 82707 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:15,981 : INFO : EPOCH 4 - PROGRESS: at 45.90% examples, 82847 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:17,048 : INFO : EPOCH 4 - PROGRESS: at 48.00% examples, 83027 words/s, in_qsize 5, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 12:03:18,095 : INFO : EPOCH 4 - PROGRESS: at 50.18% examples, 83274 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:19,211 : INFO : EPOCH 4 - PROGRESS: at 52.03% examples, 82783 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:20,319 : INFO : EPOCH 4 - PROGRESS: at 54.04% examples, 82534 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:21,326 : INFO : EPOCH 4 - PROGRESS: at 55.99% examples, 82403 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:22,527 : INFO : EPOCH 4 - PROGRESS: at 58.38% examples, 82466 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:23,603 : INFO : EPOCH 4 - PROGRESS: at 60.80% examples, 83041 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:24,751 : INFO : EPOCH 4 - PROGRESS: at 63.07% examples, 83202 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:25,782 : INFO : EPOCH 4 - PROGRESS: at 65.79% examples, 83859 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:26,791 : INFO : EPOCH 4 - PROGRESS: at 68.37% examples, 84536 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:27,875 : INFO : EPOCH 4 - PROGRESS: at 71.04% examples, 85004 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:28,906 : INFO : EPOCH 4 - PROGRESS: at 73.44% examples, 85380 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:29,933 : INFO : EPOCH 4 - PROGRESS: at 75.82% examples, 85730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:30,956 : INFO : EPOCH 4 - PROGRESS: at 78.10% examples, 85902 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:32,003 : INFO : EPOCH 4 - PROGRESS: at 80.72% examples, 86378 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:33,006 : INFO : EPOCH 4 - PROGRESS: at 82.60% examples, 86219 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:34,135 : INFO : EPOCH 4 - PROGRESS: at 85.01% examples, 86477 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:03:35,154 : INFO : EPOCH 4 - PROGRESS: at 87.36% examples, 86778 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:03:36,175 : INFO : EPOCH 4 - PROGRESS: at 90.06% examples, 87384 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:37,226 : INFO : EPOCH 4 - PROGRESS: at 92.36% examples, 87596 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:38,257 : INFO : EPOCH 4 - PROGRESS: at 94.82% examples, 87832 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:39,261 : INFO : EPOCH 4 - PROGRESS: at 96.79% examples, 87794 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:40,441 : INFO : EPOCH 4 - PROGRESS: at 99.11% examples, 87597 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:40,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 12:03:40,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 12:03:40,754 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 12:03:40,756 : INFO : EPOCH - 4 : training on 5844680 raw words (4268809 effective words) took 48.6s, 87826 effective words/s\n",
      "2019-05-03 12:03:41,772 : INFO : EPOCH 5 - PROGRESS: at 1.89% examples, 85459 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:42,808 : INFO : EPOCH 5 - PROGRESS: at 3.88% examples, 84054 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:43,849 : INFO : EPOCH 5 - PROGRESS: at 5.54% examples, 79028 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:44,945 : INFO : EPOCH 5 - PROGRESS: at 7.27% examples, 77006 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:46,070 : INFO : EPOCH 5 - PROGRESS: at 9.34% examples, 76850 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:03:47,092 : INFO : EPOCH 5 - PROGRESS: at 11.39% examples, 77984 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:48,189 : INFO : EPOCH 5 - PROGRESS: at 13.70% examples, 79901 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:49,189 : INFO : EPOCH 5 - PROGRESS: at 15.20% examples, 78088 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:50,220 : INFO : EPOCH 5 - PROGRESS: at 17.23% examples, 78606 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:51,260 : INFO : EPOCH 5 - PROGRESS: at 19.08% examples, 78318 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:03:52,319 : INFO : EPOCH 5 - PROGRESS: at 23.43% examples, 87178 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:53,379 : INFO : EPOCH 5 - PROGRESS: at 27.58% examples, 94114 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:54,398 : INFO : EPOCH 5 - PROGRESS: at 31.12% examples, 98124 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:55,505 : INFO : EPOCH 5 - PROGRESS: at 34.81% examples, 101862 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:56,537 : INFO : EPOCH 5 - PROGRESS: at 38.93% examples, 106115 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:57,544 : INFO : EPOCH 5 - PROGRESS: at 42.98% examples, 109999 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:58,566 : INFO : EPOCH 5 - PROGRESS: at 46.38% examples, 112153 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:03:59,610 : INFO : EPOCH 5 - PROGRESS: at 49.70% examples, 113555 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:00,680 : INFO : EPOCH 5 - PROGRESS: at 53.08% examples, 114671 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:01,692 : INFO : EPOCH 5 - PROGRESS: at 56.18% examples, 115258 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:02,713 : INFO : EPOCH 5 - PROGRESS: at 59.06% examples, 115486 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:03,779 : INFO : EPOCH 5 - PROGRESS: at 61.76% examples, 115385 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:04,889 : INFO : EPOCH 5 - PROGRESS: at 64.68% examples, 115137 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:06,021 : INFO : EPOCH 5 - PROGRESS: at 67.56% examples, 114564 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:07,082 : INFO : EPOCH 5 - PROGRESS: at 71.23% examples, 115701 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:08,087 : INFO : EPOCH 5 - PROGRESS: at 73.97% examples, 115669 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:09,144 : INFO : EPOCH 5 - PROGRESS: at 76.83% examples, 115701 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:10,214 : INFO : EPOCH 5 - PROGRESS: at 80.02% examples, 115922 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:04:11,246 : INFO : EPOCH 5 - PROGRESS: at 83.21% examples, 116531 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:12,285 : INFO : EPOCH 5 - PROGRESS: at 86.26% examples, 117026 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-03 12:04:13,315 : INFO : EPOCH 5 - PROGRESS: at 88.92% examples, 116671 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:14,347 : INFO : EPOCH 5 - PROGRESS: at 91.18% examples, 116083 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-03 12:04:15,380 : INFO : EPOCH 5 - PROGRESS: at 94.31% examples, 116388 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:16,404 : INFO : EPOCH 5 - PROGRESS: at 97.30% examples, 116686 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-03 12:04:17,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-03 12:04:17,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-03 12:04:17,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-03 12:04:17,229 : INFO : EPOCH - 5 : training on 5844680 raw words (4269036 effective words) took 36.5s, 117088 effective words/s\n",
      "2019-05-03 12:04:17,230 : INFO : training on a 29223400 raw words (21344061 effective words) took 190.7s, 111902 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It's for later\n",
    "\n",
    "#from gensim.test.utils import datapath\n",
    "#w2v = KeyedVectors.load_word2vec_format(datapath('downloaded_vectors_path'), binary=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7690276\n",
      "great and bad: 0.4793352\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 12:04:17,817 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('film', 0.9381517171859741),\n",
       " ('\"film\"', 0.8394811749458313),\n",
       " ('\"movie\"', 0.7739646434783936),\n",
       " ('movie,', 0.7708561420440674),\n",
       " ('movie...', 0.7672374248504639)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', 0.7421067953109741),\n",
       " ('dreadful', 0.6591118574142456),\n",
       " ('abysmal', 0.6536242961883545)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-03 12:04:18,629 : INFO : Evaluating word analogies for top 300000 words in the model on dataset/questions-words.txt\n",
      "2019-05-03 12:04:18,631 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-03 12:04:19,372 : INFO : capital-common-countries: 1.9% (3/156)\n",
      "2019-05-03 12:04:20,068 : INFO : capital-world: 2.7% (3/111)\n",
      "2019-05-03 12:04:20,221 : INFO : currency: 0.0% (0/18)\n",
      "2019-05-03 12:04:21,857 : INFO : city-in-state: 0.0% (0/301)\n",
      "2019-05-03 12:04:23,486 : INFO : family: 35.5% (149/420)\n",
      "2019-05-03 12:04:26,267 : INFO : gram1-adjective-to-adverb: 2.2% (19/870)\n",
      "2019-05-03 12:04:28,208 : INFO : gram2-opposite: 3.1% (17/552)\n",
      "2019-05-03 12:04:33,140 : INFO : gram3-comparative: 21.9% (261/1190)\n",
      "2019-05-03 12:04:36,443 : INFO : gram4-superlative: 11.5% (87/756)\n",
      "2019-05-03 12:04:39,112 : INFO : gram5-present-participle: 18.1% (147/812)\n",
      "2019-05-03 12:04:42,420 : INFO : gram6-nationality-adjective: 1.6% (15/967)\n",
      "2019-05-03 12:04:46,898 : INFO : gram7-past-tense: 19.5% (246/1260)\n",
      "2019-05-03 12:04:50,556 : INFO : gram8-plural: 7.1% (58/812)\n",
      "2019-05-03 12:04:53,024 : INFO : Skipping invalid line #19558 in dataset/questions-words.txt\n",
      "2019-05-03 12:04:53,027 : INFO : gram9-plural-verbs: 29.5% (192/650)\n",
      "2019-05-03 12:04:53,031 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2019-05-03 12:04:53,035 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2019-05-03 12:04:53,039 : INFO : Total accuracy: 13.5% (1197/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"dataset/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49285913 0.33639994 0.28832963 0.46041939 0.54495013 0.58773232\n",
      " 0.39059952 0.38276654 0.43787318 1.00420749 0.44374949 0.30750817\n",
      " 0.25738502 0.106443   0.61723799 0.60437    0.11306319 0.54095995\n",
      " 0.13626945 0.27802634 0.1526814  0.86116123 0.15953012 0.49773648\n",
      " 0.05347155 0.38054711 0.21225727 0.0208677  0.18346149 0.57532507\n",
      " 0.27736285 0.19751614 0.         0.27039531 0.18855146 1.02738535\n",
      " 0.42983744 0.54531687 0.46396169 0.23110706 0.41567695 0.77202618\n",
      " 0.25756019 0.         0.3216624  0.26563066 0.13769618 0.18036751\n",
      " 0.60689151 0.26058674 0.78789675 0.60309398 0.22167404 0.21394297\n",
      " 0.11518212 0.         0.50294024 0.54712391 0.05856852 0.47882864\n",
      " 0.40110812 0.61324054 0.39842439 0.57215089 0.3503381  0.58323079\n",
      " 0.45556119 0.29617903 0.44528612 0.42012617 0.37520707 0.28772876\n",
      " 0.         0.34326702 0.32793716 0.07953251 0.4123584  0.34720504\n",
      " 0.49595308 0.63517237 0.37723881 0.47982797 0.50306135 0.47666872\n",
      " 0.53733522 0.35785437 0.2976867  0.44710162 0.653023   0.12050198\n",
      " 0.4661459  0.36896032 0.2364924  0.41984403 0.26691386 0.75595582\n",
      " 0.53876084 0.39976859 0.32697529 0.27687547]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "\n",
    "def vectorize(text,n=100,mean=False):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "    vec=np.zeros(n)\n",
    "    mat=np.zeros((len(text),n))\n",
    "    for i in range(len(text)):\n",
    "        word=text[i]\n",
    "        if word in w2v.wv.vocab.keys():\n",
    "            mat[i]=w2v.wv.get_vector(word)\n",
    "    vec=mat.max(0)    \n",
    "    return vec\n",
    "    \n",
    "\n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text,pol in train]\n",
    "X_test = [vectorize(text) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60608"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('dataset/input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[11, 14, 16, 18, 23, 94, 10, 23, 13, 94],\n",
      "        [18, 14, 21, 13, 73, 96, 58, 14, 94, 12],\n",
      "        [96, 54, 18, 27, 73, 94, 28, 25, 10, 27],\n",
      "        [94, 15, 10, 12, 14, 94, 18, 28, 94, 15]]), tensor([[14, 16, 18, 23, 94, 10, 23, 13, 94, 14],\n",
      "        [14, 21, 13, 73, 96, 58, 14, 94, 12, 10],\n",
      "        [54, 18, 27, 73, 94, 28, 25, 10, 27, 14],\n",
      "        [15, 10, 12, 14, 94, 18, 28, 94, 15, 10]]))\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = nn.Embedding(n_char,hidden_size)\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = nn.RNN(hidden_size,hidden_size)\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arij/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 10s (100 0%) 2.5334]\n",
      "Whir wirerof f Y'stheanprd pleat make mear, w\n",
      "CENHu and mes ou fandere BO:\n",
      "Fond tallos\n",
      "PRCEned bicer h \n",
      "\n",
      "[0m 20s (200 1%) 2.4634]\n",
      "Whe's the giarithand wengisitharer, l is fathyse st lde dar ale sthe e isor RDuthe,\n",
      "Bula blouprtoare e \n",
      "\n",
      "[0m 29s (300 1%) 2.4757]\n",
      "Whenos n, n.\n",
      "\n",
      "\n",
      "N plo mUCougke, le t pour melforofaiothy:\n",
      "HI pl t bun.\n",
      "HARDen; llldoun.\n",
      "\n",
      "AS:\n",
      "Gomy IO ha \n",
      "\n",
      "[0m 37s (400 2%) 2.5063]\n",
      "Whe n thist mandof the troutror fangrso peste gnth lfat ithat copran'd t bl he tice whamalelyor?\n",
      "\n",
      "Frou \n",
      "\n",
      "[0m 46s (500 2%) 2.5449]\n",
      "Whes he tar s t.\n",
      "vionde or yowheeer atingeane?\n",
      "We imato avey y m imeive wilisharond bing beas S:\n",
      "Bur i \n",
      "\n",
      "[0m 54s (600 3%) 2.5024]\n",
      "Whim m-powan ongrrliss ter breavelf t che, h tedo n y manofth ds bames be y nod melo SLBRE:\n",
      "\n",
      "heretind  \n",
      "\n",
      "[1m 6s (700 3%) 2.5035]\n",
      "Whid\n",
      "TSThe:\n",
      "K:\n",
      "\n",
      "\n",
      "Thimupthe s apoully ty woroun benos t owingired ay s at my w the, I oue ste prok are  \n",
      "\n",
      "[1m 23s (800 4%) 2.4342]\n",
      "Whar chathourer ofl u y\n",
      "YO whave a hathes thee ake t!\n",
      "\n",
      "\n",
      "S:\n",
      "Whas ande den oupathe al bis s we ouyo bame \n",
      "\n",
      "[1m 38s (900 4%) 2.4528]\n",
      "Whor f art sthe d ain'd omir theerthe manciers has at bl o waplde marour ault,\n",
      "Whe it sache, I fo al t \n",
      "\n",
      "[1m 54s (1000 5%) 2.5362]\n",
      "Whothowhis blof thetefis d ms bu prarende wisot iket t ll y s, th sthas.\n",
      "\n",
      "BOLI te o t,\n",
      "\n",
      "Fofre dust t i \n",
      "\n",
      "[2m 11s (1100 5%) 2.4922]\n",
      "Whtore wane brer le'd cht or myowotof n re owefe, ishrs e groranshe wisormyor BUK:\n",
      "YONI manee s amo tr \n",
      "\n",
      "[2m 26s (1200 6%) 2.4835]\n",
      "Whe co bl se me tos.\n",
      "ARI'seasitchou'dingerirs LERowheny'd EShes y t sheesthe atho m, telthallo ave you \n",
      "\n",
      "[2m 40s (1300 6%) 2.4766]\n",
      "Whied mavendo whemyonouroue wind thedlisaben yo hatie meshour; heaulllin thel lle he te l VIIKIUCKEENo \n",
      "\n",
      "[2m 55s (1400 7%) 2.5349]\n",
      "Whin an d theevenge n.\n",
      "Louno.\n",
      "\n",
      "CHI y,\n",
      "ARCl an herer mes mave, om thod t, a m ou wore hy ss yay te.\n",
      "Wim \n",
      "\n",
      "[3m 9s (1500 7%) 2.5145]\n",
      "Wherree par andouinsoupis mis wicr se the ouend alesth wt'steanthe d thenous lans me!\n",
      "And thathe IOnt, \n",
      "\n",
      "[3m 22s (1600 8%) 2.4796]\n",
      "Whe d a on belose:\n",
      "ORBES ffte here t whadad h whe u mou th bee pr.\n",
      "ss u' whath bee we ind abjord thart \n",
      "\n",
      "[3m 36s (1700 8%) 2.4675]\n",
      "Whe d mu ter in itoun haveep eas ind ato saroue you hawoond,\n",
      "An:\n",
      "COLO:\n",
      "\n",
      "\n",
      "And yorew cese,\n",
      "hendo imarowe \n",
      "\n",
      "[3m 50s (1800 9%) 2.4531]\n",
      "Wheanand\n",
      "We, t the as kisthive h sthe bllllavend torilan ierdeat as.\n",
      "To\n",
      "An fotarthe buntrn Cl ng tongh \n",
      "\n",
      "[4m 4s (1900 9%) 2.4929]\n",
      "Whe me,\n",
      "Th lenareeashe,\n",
      "\n",
      "Whe wo a an witr.\n",
      "\n",
      "Thooderotharsch mou k, fovishen sthore ou s a ke RIOfel s  \n",
      "\n",
      "[4m 19s (2000 10%) 2.4851]\n",
      "Whickend wathe ISt ha\n",
      "I homes a whean t he gousche t t n anofre An t lin lloules th ullour the mer m.\n",
      " \n",
      "\n",
      "[4m 35s (2100 10%) 2.4382]\n",
      "Whesthinopaior:\n",
      "Finduthe bl p\n",
      "Bore thur s ce I ave anot the t whodoume fet bio ille t bores h fourer i \n",
      "\n",
      "[4m 50s (2200 11%) 2.5022]\n",
      "Whe,\n",
      "NTRDI psthitatheeve he d hif therotito s t t t llin n h s mpurt thain myowers thipofle mak hithr. \n",
      "\n",
      "[5m 4s (2300 11%) 2.4513]\n",
      "Whimore ars the me weachowore dit,\n",
      "IUThin, ler LUThir IO:\n",
      "TINI mer ur tht t n lor te the athe io tre e \n",
      "\n",
      "[5m 19s (2400 12%) 2.4973]\n",
      "Whengulllom.\n",
      "RENGoourofo Ind terin ge core, hive pe me thie, heanoting me nicores e s sel brend t\n",
      "\n",
      "GLA \n",
      "\n",
      "[5m 33s (2500 12%) 2.5599]\n",
      "Whereshe: caleld ffo INGo scerarsed t hen te tofoune methothoof cue is s thalend nothintheachan tondou \n",
      "\n",
      "[5m 48s (2600 13%) 2.4795]\n",
      "Wh't t bere ughin nskng, y hy, f I ind\n",
      "ARENor KENINTIUS:\n",
      "\n",
      "NG pot, tharonengand\n",
      "Y win;\n",
      "Bufor akingatim  \n",
      "\n",
      "[6m 4s (2700 13%) 2.4548]\n",
      "Whouno my athin, bancecea be higeethothe am.\n",
      "We rowowainorir woust p t s n fom the thaim ire IO:\n",
      "'Thad \n",
      "\n",
      "[6m 18s (2800 14%) 2.5069]\n",
      "Wht t thabellitho ouren th than s isold,\n",
      "\n",
      "Myoust, m y y es we and,\n",
      "TIO:\n",
      "An y g horsllt h ange hand a l \n",
      "\n",
      "[6m 26s (2900 14%) 2.4924]\n",
      "Whart th t bouce, omud s t s tofa,\n",
      "ES:\n",
      "\n",
      "My frse womantheat y than, Whint sef the s\n",
      "A: thons histitt t  \n",
      "\n",
      "[6m 34s (3000 15%) 2.4712]\n",
      "Wheman mabur w tinthe we aseer.\n",
      "An te co yome hay Bur h,\n",
      "Bury n, ig ant cher te s we ik:\n",
      "RALAnilosorir \n",
      "\n",
      "[6m 42s (3100 15%) 2.4390]\n",
      "Whindere be thello r w!\n",
      "\n",
      "T:\n",
      "Twin thouro scor hesou jue ha s, h t\n",
      "Canald bard lll sthith thas.\n",
      "The wime \n",
      "\n",
      "[6m 50s (3200 16%) 2.4331]\n",
      "Wh s le se wond werer'lene?\n",
      "YO, terind fand s CKEd\n",
      "IN t thead be t,'she.\n",
      "I alre fowanct thas wnd or ha \n",
      "\n",
      "[6m 58s (3300 16%) 2.4672]\n",
      "Wheengngber;\n",
      "\n",
      "Ithe be ilavego l thor d agou he mouritowe; ove, p,\n",
      "KEn:\n",
      "\n",
      "ANond\n",
      "The.\n",
      "D:\n",
      "The nd tronanor  \n",
      "\n",
      "[7m 5s (3400 17%) 2.4566]\n",
      "Wh shered cke meand d fe me, his t,\n",
      "Tillld se, ghe an keaberor, ouleid LO:\n",
      "\n",
      "Fasice s;\n",
      "Frthodoneand pre \n",
      "\n",
      "[7m 13s (3500 17%) 2.4605]\n",
      "Why wnd tinive t his, heng d bel, Thotee he, thind hat mere s ha my atheau hest y be l bl.\n",
      "O de r we g \n",
      "\n",
      "[7m 23s (3600 18%) 2.4165]\n",
      "Whe a wo as\n",
      "Thy theave ctou ce t ad esof ' mean bldurth omure ce ar t be stho hearers's wistous d mee  \n",
      "\n",
      "[7m 31s (3700 18%) 2.5028]\n",
      "Whe w ore Y r le the an, edio y ble heretr onouror beyo coures brallor fou thillineape an'thefo thear  \n",
      "\n",
      "[7m 40s (3800 19%) 2.4603]\n",
      "Whe'dit o, sthengrat bu as ser aveande leart the t\n",
      "Hat shatheand moos, s;\n",
      "Angris y.\n",
      "HARI t t matee sh  \n",
      "\n",
      "[7m 48s (3900 19%) 2.5044]\n",
      "Whis fardomed myot hithizerarofand the t d se ake y:\n",
      "Hellitick whond thes beld mo wirdor st ce is nes  \n",
      "\n",
      "[7m 57s (4000 20%) 2.5063]\n",
      "Whend's be y presofonce hicethithere?\n",
      "Anves,\n",
      "TI teealllds,\n",
      "Whoncand heres ait cithe he ther ce bearomp \n",
      "\n",
      "[8m 5s (4100 20%) 2.4895]\n",
      "Who mordand ICou,\n",
      "MIV: cilll m nithe,\n",
      "Wherr mer 'Tist serel weanoprerin thor t t, canthende che, igere \n",
      "\n",
      "[8m 13s (4200 21%) 2.4778]\n",
      "Wht mo ge, t s o h sthegr in the,\n",
      "He amance se ber o g E a w res teal ed ay ws st t;\n",
      "'s\n",
      "\n",
      "TENoor sserwe \n",
      "\n",
      "[8m 21s (4300 21%) 2.4351]\n",
      "Whandon it t my ten theanothind the ongheathit,\n",
      "Wheatharsty y st was mfedg orbes berie hico hat thy in \n",
      "\n",
      "[8m 29s (4400 22%) 2.4109]\n",
      "Why watoriond ing nicate t ho an wousourve t as weath, at ierorefitan-cot iar u, cthanordeacllot lllot \n",
      "\n",
      "[8m 39s (4500 22%) 2.4358]\n",
      "Wh hedy thirmak'Tin ne I's aind.\n",
      "K:\n",
      "Tono wer t m mosthe'soke, t hese te cow.\n",
      "LIARI her. IA GLIARitlin  \n",
      "\n",
      "[8m 47s (4600 23%) 2.5055]\n",
      "Whters. ds thembrimyo ouclousha llan bine oryowhateas theat k'thathenghenoveran s! te seet he marerd a \n",
      "\n",
      "[8m 56s (4700 23%) 2.4267]\n",
      "Wher nd Gle himer t sh yshughe thareasu n\n",
      "\n",
      "\n",
      "Toure wn h ha IOLI fe athict ie y thenghachisthird IAnd un \n",
      "\n",
      "[9m 5s (4800 24%) 2.5166]\n",
      "Whithut hane.\n",
      "Tompe alle tserd w, f,\n",
      "\n",
      "O o h the tseavere, INTERIOHAne arllfe!\n",
      "Fogn my ofe haspenf ben  \n",
      "\n",
      "[9m 13s (4900 24%) 2.4917]\n",
      "Whe wisenystshe,\n",
      "\n",
      "MIZEThed s\n",
      "Pr, we irlean ce 'd a ar ssh f If y aked g My thire Glol be thest yo t sl \n",
      "\n",
      "[9m 22s (5000 25%) 2.4450]\n",
      "Wh foutot te tedothoulanthang bererss thos\n",
      "\n",
      "Ase anie t, nde sle th thald?\n",
      "\n",
      "\n",
      "II s tharelothalorthe ware \n",
      "\n",
      "[9m 29s (5100 25%) 2.4205]\n",
      "Wheangatrs gbus wiul bevenere s wosore llderer me, ee ing linder thenofo's we RES:\n",
      "LAs baind wse at f  \n",
      "\n",
      "[9m 37s (5200 26%) 2.4600]\n",
      "Whilincaly t BA:\n",
      "Plisiste monsthe willllfoungou, thape Angeristhe, I brofu homaro th g. t thinouangr b \n",
      "\n",
      "[9m 45s (5300 26%) 2.4494]\n",
      "Whidake k, INTERO:\n",
      "OMy haret lof as h il ITRUCHARCHer, wh d s s,\n",
      "S:\n",
      "ARD at s hthen mat hthowe my and a \n",
      "\n",
      "[9m 52s (5400 27%) 2.4517]\n",
      "Whine ile w, hat ily bil thin nd illllathall noulith,\n",
      "Wh,\n",
      "HAnee ar alat, s glo s.\n",
      "Se ore Whe, be he al \n",
      "\n",
      "[10m 0s (5500 27%) 2.4885]\n",
      "Whint tinould me ld\n",
      "\n",
      "I chanry brs ancel imaitellar ste t yo.\n",
      "Thafr ak,\n",
      "Ind caim inenr:\n",
      "AUT:\n",
      "\n",
      "\n",
      "MEvingre \n",
      "\n",
      "[10m 8s (5600 28%) 2.4806]\n",
      "Wher thatheneeroon ll hy\n",
      "T: than wet t f te ld\n",
      "I me wnerand trf toad atou\n",
      "Cl I hinyo llorecostheng tou \n",
      "\n",
      "[10m 18s (5700 28%) 2.5277]\n",
      "Whoth ceer th higes IARYose aindis, he he inchem, t,\n",
      "Whad l ivess awer's bllid, a th de anorarnd band  \n",
      "\n",
      "[10m 28s (5800 28%) 2.4559]\n",
      "Wh t bed HALLOLOurdes ws her thad winok; g.\n",
      "ORO:\n",
      "PUCUCatithit towire thes thend whisathomy o wird, he. \n",
      "\n",
      "[10m 41s (5900 29%) 2.4563]\n",
      "Where ises as\n",
      "Bu my ad, mo, of boliswror wondencker by welllly hajorotrease If tono be the nothor stho \n",
      "\n",
      "[10m 58s (6000 30%) 2.5096]\n",
      "Whur cas cherghereen o thake non m ou s ant hak akel y wher f sthayo ware se mend anfin:\n",
      "Whe ak he h t \n",
      "\n",
      "[11m 17s (6100 30%) 2.4144]\n",
      "Whe a athear,\n",
      "\n",
      "ANCE ber d r ws t d wh t d d sthis our?\n",
      "Blam nd'Thind meare y\n",
      "AUMyom tr ple bun cay det \n",
      "\n",
      "[11m 33s (6200 31%) 2.5026]\n",
      "Whe dlod at vew hofo be who woule,\n",
      "BENDo s e,\n",
      "Thawinon t seve t, y alonthild y.\n",
      "LI pa s s t haithourer \n",
      "\n",
      "[11m 49s (6300 31%) 2.5052]\n",
      "Wheme, wome h it and a alicomu sive t hyo torome be toy, spr.\n",
      "Myoverethinceane heig mp mee, thed wence \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12m 6s (6400 32%) 2.4650]\n",
      "Wh to her s ' s f cheagemann The be ustous hiveee as?\n",
      "Pr thearyo, ICounof pllld, tofoplfamee ad mard s \n",
      "\n",
      "[12m 22s (6500 32%) 2.4828]\n",
      "Whond--d s thand we I f te I m myisend e bowane t aney d agst NG y imome tofothepe ir clisthingaslerd  \n",
      "\n",
      "[12m 35s (6600 33%) 2.4724]\n",
      "Wht Cathans thas de mer, hand ldy le s anend, rnover S:\n",
      "To'd-mame, t he st thand mannnind hy my, whigo \n",
      "\n",
      "[12m 51s (6700 33%) 2.4668]\n",
      "Wher cr ha let owis thout fod, the d cke wis.\n",
      "Wen t.\n",
      "Whes at y prthot te f\n",
      "G g is the nofothouct beath \n",
      "\n",
      "[13m 6s (6800 34%) 2.4589]\n",
      "Whavad heree blle mon'ticasithema whe arat kere mind therthat sthe andsureld Youtis bo y, ve stistho o \n",
      "\n",
      "[13m 20s (6900 34%) 2.5296]\n",
      "Wheteang, minovoroned hangrupr my ou ouls you, o,\n",
      "Thise ss tioworuthat henowilo Fit y arerer the wou h \n",
      "\n",
      "[13m 34s (7000 35%) 2.5007]\n",
      "Whonthe\n",
      "Mat O:\n",
      "Me be werd f yo tes ts the rt coumeaim rour o araroun wit thaiss the ll anden,\n",
      "CENGoll  \n",
      "\n",
      "[13m 47s (7100 35%) 2.4341]\n",
      "Wha the pre n gen t at owe we and wore s se me my athaweerd bur wh s ffo wofond, haun haras the corist \n",
      "\n",
      "[14m 2s (7200 36%) 2.4650]\n",
      "Whitheat t thesthe ad th ak me itol lthandstharese t ly thoudo t l:\n",
      "INIO, the pe at thouslles mo w.\n",
      "Th \n",
      "\n",
      "[14m 16s (7300 36%) 2.4610]\n",
      "Whthe ple herd he us at we pour to itins!\n",
      "Whithare ingerowincingosthad hed he th w fa 's atht haco ayo \n",
      "\n",
      "[14m 29s (7400 37%) 2.4767]\n",
      "Whe s herthe m my blyoubeand thathe rerelive n RI meif pre meng wes theleco so wis, ee s may anoker t: \n",
      "\n",
      "[14m 43s (7500 37%) 2.4674]\n",
      "Wh wener mo and ICIUCuneng.\n",
      "WAn medand\n",
      "YCENENGETI'larerel cee mat he ou ilad teseral be t s\n",
      "S:\n",
      "II:\n",
      "Wha \n",
      "\n",
      "[14m 57s (7600 38%) 2.4639]\n",
      "Whe\n",
      "I u I d heer he aneff tou wowandesernthacerave pseouthe me wear ar my ancon t f;\n",
      "Whend,\n",
      "Clo d y s  \n",
      "\n",
      "[15m 10s (7700 38%) 2.4579]\n",
      "Whe's istonde tono inde y elde hio f th stus im owht t mas nomot the fo:\n",
      "Un:\n",
      "NG lourace therd ma othe  \n",
      "\n",
      "[15m 24s (7800 39%) 2.5241]\n",
      "Why oursas s f RKIN:\n",
      "A ace t uis war hol st ded w;\n",
      "HAnondisseacre t cous heris, theanon is y f n d hat \n",
      "\n",
      "[15m 39s (7900 39%) 2.4888]\n",
      "Whe bllese ithe, I y tane arellalthe thoul thobeose ware ar,\n",
      "Fig nd, wim the d in him.\n",
      "\n",
      "T:\n",
      "F this spr. \n",
      "\n",
      "[15m 52s (8000 40%) 2.4537]\n",
      "Whavitot ur;\n",
      "\n",
      "\n",
      "Bulouepilereare lit t sil amourempre phowe pr?\n",
      "O:\n",
      "UMIUS:\n",
      "Gr nd mat d, o I y r,\n",
      "\n",
      "LEThir  \n",
      "\n",
      "[16m 6s (8100 40%) 2.4607]\n",
      "Wh uenothandrnor mimp, cesl;\n",
      "\n",
      "CHind, satthe hecomerdea felfousthatan at whincom wanorn cund mainoratha \n",
      "\n",
      "[16m 16s (8200 41%) 2.5099]\n",
      "Whilo hele br n, mend nouth ng, whe s d.\n",
      "I thed alllet fousthe hithachisthe we.\n",
      "An.\n",
      "g handr bre mathen \n",
      "\n",
      "[16m 24s (8300 41%) 2.4455]\n",
      "Whe\n",
      "I rorano man e?\n",
      "An aned STord t the t motin; f mexeare and m, then ur:\n",
      "He:\n",
      "Gag; s owen w, molllf P \n",
      "\n",
      "[16m 31s (8400 42%) 2.4610]\n",
      "Wh ce, the ll m m ce sof me hetasadish t thod were, d\n",
      "\n",
      "Fove oupl couchive f RD:\n",
      "An, sthe th po foon l  \n",
      "\n",
      "[16m 39s (8500 42%) 2.4302]\n",
      "Whed tanould the t Whis:\n",
      "T:\n",
      "Be h my lmf be d Thishe meray chit hancowe my har the ar-rerme a healmeas  \n",
      "\n",
      "[16m 47s (8600 43%) 2.4930]\n",
      "Whes my migasbo sesowoir thate!\n",
      "Hed ngadit, y nd t thathing t, b; l fiseryonowe, f ckethedise mand puc \n",
      "\n",
      "[16m 54s (8700 43%) 2.4891]\n",
      "Whes s an,\n",
      "Nar coulind wnd whicheretthor t thine her lo te he d's t, wigurat me hinderangso'ss thait g \n",
      "\n",
      "[17m 2s (8800 44%) 2.3696]\n",
      "Whedoulit\n",
      "s sllthe fowe y.\n",
      "\n",
      "Can l nthin?\n",
      "\n",
      "Fon thexetindesmere wis w bld torer be' thenongrd f mas s'du \n",
      "\n",
      "[17m 9s (8900 44%) 2.4467]\n",
      "Whoothan hancesitor wicot dotholiatr thar henje the y\n",
      "\n",
      "Thistoul linsthes, ato my in ald lemale aked my \n",
      "\n",
      "[17m 17s (9000 45%) 2.4898]\n",
      "Whe y se they thapean theceathe INGeno tr she fougelles.\n",
      "\n",
      "Arand MO:\n",
      "\n",
      "\n",
      "Whouplldethendingomat, blen;\n",
      "\n",
      "\n",
      "A \n",
      "\n",
      "[17m 24s (9100 45%) 2.4478]\n",
      "Wh se\n",
      "Ofrourun me fond m t bive is ter s t anosanit?\n",
      "MIUClle olll w, fend CAngores; arin He, s tothono \n",
      "\n",
      "[17m 32s (9200 46%) 2.5019]\n",
      "Whowhand m furt macis meave the a hour wn atly thandsthomy thou, the ldes.\n",
      "\n",
      "INe hou shel, ms, and\n",
      "\n",
      "\n",
      "CE \n",
      "\n",
      "[17m 39s (9300 46%) 2.4740]\n",
      "Whounind, wary fo t ueche pe ay nghenfoutofint f f I than, ongond y: st Thene sharere lof as he tather \n",
      "\n",
      "[17m 47s (9400 47%) 2.4658]\n",
      "Whe.\n",
      "Cl the pe the he ur,\n",
      "S:\n",
      "As on. fo amucan mathe cer d ind t t y an havithateyof thomer:\n",
      "\n",
      "An hy wel \n",
      "\n",
      "[17m 55s (9500 47%) 2.3898]\n",
      "Wha hilathin Mas of wainge, e,\n",
      "O:\n",
      "Sh a:\n",
      "\n",
      "\n",
      "The ling s tanow, gankes s han wed cor'den gisimave faingers \n",
      "\n",
      "[18m 3s (9600 48%) 2.4560]\n",
      "Whinthe, yowaceiestrieare itingoyowhancomig INowere ro the.\n",
      "\n",
      "\n",
      "S:\n",
      "\n",
      "\n",
      "MICO:\n",
      "CARO:\n",
      "She ars mengr s boun l  \n",
      "\n",
      "[18m 11s (9700 48%) 2.4552]\n",
      "Whellllinghe tin t s.\n",
      "\n",
      "\n",
      "ARULAnd h usur te t h th idiervere g ho y thea tougan heel mo he ais hin d, h  \n",
      "\n",
      "[18m 19s (9800 49%) 2.4023]\n",
      "Whed hatheresowiss p w ore ime ste ts way sofathe wathe walyed pe thor.\n",
      "Go I mouling thit tu llon make \n",
      "\n",
      "[18m 27s (9900 49%) 2.4842]\n",
      "Wh MNClis, I tryouronis navou wen' yore horod beney bes cethk me s sthere d thont her has\n",
      "A ornd t y f \n",
      "\n",
      "[18m 35s (10000 50%) 2.4949]\n",
      "Whawr thand fof t t'd pr, se fo atou il ther t mene?\n",
      "PUS:\n",
      "He mau anouraste arerans t f 'leare mif th f \n",
      "\n",
      "[18m 43s (10100 50%) 2.4960]\n",
      "Wh arendindourd s t ENG llanomiserin lds hatino the t.\n",
      "\n",
      "HAUEHAMave on t.\n",
      "Why tu be n n hin thimbe d t  \n",
      "\n",
      "[18m 50s (10200 51%) 2.4203]\n",
      "Where, I hilllu ise h fere w t gler t blot thie sieranthado heee nd gesthe th t Beckere, CARY:\n",
      "O:\n",
      "AR:\n",
      " \n",
      "\n",
      "[18m 58s (10300 51%) 2.5264]\n",
      "Whe all s be d t hea y ding d, ake?\n",
      "ARENI nd my he lld, Lotala at o my h\n",
      "Andndshereewe hero morathast, \n",
      "\n",
      "[19m 5s (10400 52%) 2.4366]\n",
      "Wher:\n",
      "\n",
      "Siveckentheanthey woure ayor, ay he s isese lourere amyons we, tavir, s t, her han whe s byesof \n",
      "\n",
      "[19m 13s (10500 52%) 2.4559]\n",
      "Wherrouro alaveag d ie, y me an D fe'd hes, atiernes therer as cemam.\n",
      "THerer and chastconen h, weise d \n",
      "\n",
      "[19m 20s (10600 53%) 2.4498]\n",
      "Whathen s, t;\n",
      "Kite m, l bearn att whowathit.\n",
      "\n",
      "\n",
      "My,\n",
      "Tr me maino t. y ves asor wis t y t at t fr'\n",
      "Thy ar \n",
      "\n",
      "[19m 28s (10700 53%) 2.4469]\n",
      "Wher ceas d s as.\n",
      "ARI ar ave ssean wintt g ty s wiclomillino wond, I\n",
      "t dakeseren ive, uldos feer fo oo \n",
      "\n",
      "[19m 35s (10800 54%) 2.4498]\n",
      "Who tha any bllint'd your,\n",
      "\n",
      "\n",
      "ARD:\n",
      "\n",
      "Ho, hang 'leachit ct t lo, aswnf bat CENo thave I as wailsouss t as \n",
      "\n",
      "[19m 43s (10900 54%) 2.4098]\n",
      "Whe t s mere s,\n",
      "\n",
      "Bithay batatorr ait ad ithilt, by t ise Ththy s merte waren s at--d, toutethillinou o \n",
      "\n",
      "[19m 50s (11000 55%) 2.3984]\n",
      "Wh awa pawsto twid, gearinouther tharkere han owove herand\n",
      "Sin mene be heithen\n",
      "Hombeinoureemince ond t \n",
      "\n",
      "[19m 58s (11100 55%) 2.4890]\n",
      "Whandowavire odoveithas! farack aver goure.\n",
      "Wearithicth; apre wn hy s pe, llond mus ENThe fed s oris a \n",
      "\n",
      "[20m 5s (11200 56%) 2.4468]\n",
      "Wheshorou irou thancerexp the thor n:\n",
      "\n",
      "Wid ierd henoove fos d theaul hangr ioulamises ham oure he avar \n",
      "\n",
      "[20m 13s (11300 56%) 2.4568]\n",
      "Whingillesthin s te scenome mithitheey hithey in byout beanimere t w te an that thele co ce s lontroth \n",
      "\n",
      "[20m 20s (11400 56%) 2.5138]\n",
      "Whemne p wat thid hinge,\n",
      "RCO:\n",
      "\n",
      "ENCENThtheemy, thakes tieth'dep Yomafr t hemelesouint tau hiff ENToow's \n",
      "\n",
      "[20m 27s (11500 57%) 2.5015]\n",
      "Whaicorces itorot he t cem, ier fenourber trin t tourearerarir imard inofrour serreemereder wewis thia \n",
      "\n",
      "[20m 35s (11600 57%) 2.4870]\n",
      "Whis I fathatay angnanghis land\n",
      "Gotang hererrthad, odinyo, and s is mar ndise wif alar, ay, ofle a fr  \n",
      "\n",
      "[20m 43s (11700 58%) 2.4786]\n",
      "Whol 's twin wes ang I in this, nd ourotr po t\n",
      "Y arde, rthe\n",
      "Gororarerthiru ine the t ber thare:\n",
      "GHAn t \n",
      "\n",
      "[20m 51s (11800 59%) 2.4825]\n",
      "Whall th senon he l r beventhe asithak'd o\n",
      "TAn t irely ar the\n",
      "ERO, her.\n",
      "\n",
      "Go'lawhifonetet ap ll hacr te \n",
      "\n",
      "[20m 58s (11900 59%) 2.4501]\n",
      "Whanowis my thelk t:\n",
      "A:\n",
      "ORA:\n",
      "Who t disbaind nonois y anknsemitous beatornge s s myomoures perend BO: n \n",
      "\n",
      "[21m 6s (12000 60%) 2.4565]\n",
      "Whanous t yothimethabed.\n",
      "tang imiteait mangndly,\n",
      "BR:\n",
      "SThidind t bat the win.\n",
      "Youn O:\n",
      "NI n at lllliveat \n",
      "\n",
      "[21m 13s (12100 60%) 2.4697]\n",
      "Wheay INCars jomyomad go e Mad h tthe bofis frest ar the w s outhey\n",
      "DInd asod thepor, gon out t blou i \n",
      "\n",
      "[21m 20s (12200 61%) 2.4896]\n",
      "Whyouroulin inou sod ff f hithen te bllar, shy teran t pextrnd tif sproushe urtonoutwithilotetene.\n",
      "The \n",
      "\n",
      "[21m 28s (12300 61%) 2.4569]\n",
      "Wh aico t coon ase d, thers sou be tewacou the, and,\n",
      "Thothathes\n",
      "Th heatruth shind beyove ameind fiommu \n",
      "\n",
      "[21m 35s (12400 62%) 2.4353]\n",
      "Whiom he, s, t, msit tethau bese there sourct g hay see s wheay with thel hase, u an cot h l me hen st \n",
      "\n",
      "[21m 43s (12500 62%) 2.4888]\n",
      "Whauar searcerthengeaimar at d, th,\n",
      "Hiour, thrathe f yomy,\n",
      "Whorer prene me thatle methee se whayo be n \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21m 50s (12600 63%) 2.4251]\n",
      "Whey o vernd e thery, we in t my:\n",
      "\n",
      "\n",
      "IFLELe s is bar ofof Re chy put atuastind be tinenelend l the the  \n",
      "\n",
      "[21m 57s (12700 63%) 2.4692]\n",
      "Whee therth 's t\n",
      "\n",
      "INROKAnord foriererkngound IO cesang rthes, ind sterur thereat,\n",
      "\n",
      "HArindgal mathe.\n",
      "\n",
      "T \n",
      "\n",
      "[22m 5s (12800 64%) 2.5019]\n",
      "Whiol ge fon'd pon m. d'sherofen t tha om soouree.\n",
      "Tangha tha w's t'thy sa folerend o!\n",
      "\n",
      "TI arl hemy, u \n",
      "\n",
      "[22m 12s (12900 64%) 2.4470]\n",
      "Whice ossth owin'l BES:\n",
      "\n",
      "O:\n",
      "\n",
      "ORAn h is I my, be'd malfteak nd he isctothatck, pt toulanound whast INET \n",
      "\n",
      "[22m 20s (13000 65%) 2.4354]\n",
      "Wher we I owar l woron owe me lis woul INo th stweatondof merthet ain o arulak,\n",
      "\n",
      "\n",
      "Ang th alld th th ce \n",
      "\n",
      "[22m 27s (13100 65%) 2.4438]\n",
      "Whandetome ad thandrave, buthe anges onde hrisoo ais d santheathe h wengar's trdenong shange ay thaven \n",
      "\n",
      "[22m 35s (13200 66%) 2.4802]\n",
      "Whththat o atood tre sst t mecoo I l.\n",
      "\n",
      "\n",
      "O:\n",
      "Whashithe.\n",
      "IVOR:\n",
      "Al mon bs skew se n:\n",
      "Fouthrowesthothid.\n",
      "HE \n",
      "\n",
      "[22m 43s (13300 66%) 2.5058]\n",
      "Whte ss yor?\n",
      "\n",
      "MEve asthy cows sthes or miard tre cthareagrd\n",
      "Thet thor torraill a the toure t s, E kird \n",
      "\n",
      "[22m 51s (13400 67%) 2.4415]\n",
      "Whothe cuere s tre tut theat barsistwasow ty toul fomy me at bue t;\n",
      "\n",
      "CK:\n",
      "I hese ome t t thar vee haves \n",
      "\n",
      "[22m 58s (13500 67%) 2.5413]\n",
      "Wher sst s we sthe f t fank,\n",
      "\n",
      "HE we\n",
      "\n",
      "Cau ayo hectherer, t pred, r:\n",
      "L: s m mentishicans m.\n",
      "Bu HERYe in  \n",
      "\n",
      "[23m 6s (13600 68%) 2.4420]\n",
      "Whe gonlde th my so anomout ise the th me heanch beshang blll,\n",
      "Burid 'stimef s or an BO:\n",
      "Son sou bu cl \n",
      "\n",
      "[23m 14s (13700 68%) 2.4908]\n",
      "Whe le l ad ot whanovime irst wane I ys th berod tthe I atheas iveceand fer prse wera wr, slllllve cho \n",
      "\n",
      "[23m 22s (13800 69%) 2.4168]\n",
      "Whalithonther oocou,\n",
      "Than eran malll w cat is shou power he pepathe?\n",
      "\n",
      "CHA:\n",
      "\n",
      "Whe he bre thanick?\n",
      "Thalle \n",
      "\n",
      "[23m 30s (13900 69%) 2.5078]\n",
      "Wharowe wh farehid bere ce d ave.\n",
      "MENI averwe t br y owe iveery mar he il mesan:\n",
      "\n",
      "\n",
      "\n",
      "ANI tho y od th se \n",
      "\n",
      "[23m 38s (14000 70%) 2.4224]\n",
      "Whe, fecethorer bedg s tisononoul the d al d'lour nd thaches r le whyoorel terou wichan coos, min,\n",
      "K:- \n",
      "\n",
      "[23m 46s (14100 70%) 2.4422]\n",
      "Whe pl me! ameninthe m s;\n",
      "\n",
      "S:\n",
      "wimou se os t hispiseel'd, w whee IONalide mspu,\n",
      "\n",
      "I wate e yener m IUSis \n",
      "\n",
      "[23m 54s (14200 71%) 2.4881]\n",
      "Who's t ppand,\n",
      "Thid hathesithiont avencoon whed sthenind ty t lureat youngheareaner gatho y toysthinck \n",
      "\n",
      "[24m 2s (14300 71%) 2.4461]\n",
      "Wher t t ing ashotik'thave inccanemes kne t m satind, t wad teand sis trer h th. be ore pes ord hofand \n",
      "\n",
      "[24m 10s (14400 72%) 2.5266]\n",
      "Whowicongaze atusther myot sere d wathathileldedad to. an, arur, nt r,\n",
      "CLe arent:\n",
      "Anthinowin res, thit \n",
      "\n",
      "[24m 18s (14500 72%) 2.4589]\n",
      "Wh alld s ima me hen\n",
      "ARE theiondere an thay hee then h ond hatheleanicis othereno t iar coulid:\n",
      "\n",
      "I the \n",
      "\n",
      "[24m 25s (14600 73%) 2.4653]\n",
      "Whard s by as e herereee sh theret, ve aghebes ces f ingh, word I the, d, ounke he hit bert.\n",
      "The by to \n",
      "\n",
      "[24m 33s (14700 73%) 2.4948]\n",
      "Whan plolll;\n",
      "KIO: t whtu thak her bind.\n",
      "It there y st s y.\n",
      "OLUSAn he be nes ll y tt alan ousuto lavesi \n",
      "\n",
      "[24m 41s (14800 74%) 2.4952]\n",
      "Whtharapr pove\n",
      "\n",
      "\n",
      "TIO:\n",
      "THAnd: h wo mer d, fod, Bes iry a ll hee husastheveopid thame mptho h ncrer him, \n",
      "\n",
      "[24m 49s (14900 74%) 2.4528]\n",
      "Whabese menghomy wngou wn f 'san y pilyoughee ou astres t hot was t s wours?\n",
      "CEO:\n",
      "MARDenceroy aner the \n",
      "\n",
      "[24m 57s (15000 75%) 2.4477]\n",
      "Whar inordouthid mire iting,\n",
      "Whifor y thint wond f be d INGourethe wilditrery tendour t a kery kenofon \n",
      "\n",
      "[25m 6s (15100 75%) 2.5255]\n",
      "Whereno in ndellond? bld yon n n:\n",
      "Alid ad po at uand ath we, nd, tour\n",
      "Tot 's.\n",
      "His tet o st atalered t  \n",
      "\n",
      "[25m 13s (15200 76%) 2.4642]\n",
      "Whalstwisen thomere arenchitro, o howalls pount ingir ndend y in bis bemald thal uso hef ald\n",
      "We s d gh \n",
      "\n",
      "[25m 21s (15300 76%) 2.4157]\n",
      "Wh ICEMangrach s it hider t therote br he aldalenge t tharsthor chencoupit\n",
      "As in pathoresther hinoinol \n",
      "\n",
      "[25m 29s (15400 77%) 2.5097]\n",
      "Whathinord, y thar mantthe s resarrun cre.\n",
      "che:\n",
      "Thais h theatheagond k, fen wes be th michare face th  \n",
      "\n",
      "[25m 36s (15500 77%) 2.5175]\n",
      "Whererouno w museinofoo o wisat cthe ner orimyowe bou bututh t mas I y, s lut bl ind istit wen thas th \n",
      "\n",
      "[25m 43s (15600 78%) 2.4313]\n",
      "Whind s oth, touren anineavencand the n d the s je y t spe s my le RUKI'showhofet itr t hir awo s h 's \n",
      "\n",
      "[25m 51s (15700 78%) 2.4533]\n",
      "Wh ndre we watren our s t foule'd cowepel houest,\n",
      "Win thesthanghestholan:\n",
      "\n",
      "\n",
      "Be thorve.\n",
      "YO: I Whes pe y \n",
      "\n",
      "[25m 58s (15800 79%) 2.4999]\n",
      "Whie ce no t w ano chie ome mot h ourore whe INOLARigave or thal tor?\n",
      "Fr an:\n",
      "I:\n",
      "Foldellllomy t or me t \n",
      "\n",
      "[26m 6s (15900 79%) 2.5035]\n",
      "Whabes hatou hil d or we\n",
      "NGBur frencowheer sererce pe y thaphe?\n",
      "LAn y othanor the inshenooulis le then \n",
      "\n",
      "[26m 14s (16000 80%) 2.4830]\n",
      "Whe use o at me whorkeag he m:\n",
      "\n",
      "\n",
      "\n",
      "I te l, al t thusour aun he s heve ar s timana man phine he comome a \n",
      "\n",
      "[26m 22s (16100 80%) 2.4457]\n",
      "Whethinde go fothitril whe ke,\n",
      "Burou int. itho myeierend, tes\n",
      "LINCome coutherd:\n",
      "Fit winge tsenou be, t \n",
      "\n",
      "[26m 30s (16200 81%) 2.5236]\n",
      "Whiter embe bano d, be somill\n",
      "LLIORUCin nd s mo!\n",
      "WAUS:\n",
      "Ficoun,\n",
      "T:\n",
      "ABUS: t whicline ath kere? in lor th \n",
      "\n",
      "[26m 38s (16300 81%) 2.4365]\n",
      "Whand foure got:\n",
      "\n",
      "G hincherathiathe f akidlild whe, hier thel m rat blt cow his t hear fouseyengsh sth \n",
      "\n",
      "[26m 46s (16400 82%) 2.5453]\n",
      "Whearomyind wns sty, s he t ofe yo, w athit e be hithutherosesthe peeraucke the t at be\n",
      "Shrer.'d sallo \n",
      "\n",
      "[26m 58s (16500 82%) 2.4890]\n",
      "Whear h forouthe I st ICollllsst tind tound toreme I'tr AUS:\n",
      "The t e ch y meren heruthancr ochome cout \n",
      "\n",
      "[27m 12s (16600 83%) 2.4798]\n",
      "Wh imars, iththilinsut t do atir toure s is whet hat dg tises mer there, ut hive hell an ware acous mi \n",
      "\n",
      "[27m 27s (16700 83%) 2.4581]\n",
      "Whomer nd sheaishillm,\n",
      "ar thet as thed,\n",
      "\n",
      "\n",
      "HAn pag thenonthegomy, irerse blound t alanckigho pamat pe m \n",
      "\n",
      "[27m 43s (16800 84%) 2.4558]\n",
      "Whaccan cexe,\n",
      "ANGO: nthid brl s th th d loreal ithie g he MENGABous, ben t ongappathin t f yos atcad i \n",
      "\n",
      "[27m 56s (16900 84%) 2.4139]\n",
      "Whad where, maro y bar qutrming bele wo w,\n",
      "O:\n",
      "\n",
      "O: t nour iloreer Fon el IOLothes in s t, at w Mathe s  \n",
      "\n",
      "[28m 12s (17000 85%) 2.4712]\n",
      "Whetal tald the?\n",
      "He n t k e?\n",
      "N:\n",
      "Angand as w prd mbe of had.\n",
      "KI tof y.\n",
      "Hemalyoouthitot llour al t nd d  \n",
      "\n",
      "[28m 25s (17100 85%) 2.4109]\n",
      "Wheis wans torayorathemyonour t ayof bu hanotofe mavawinturdor me me th thon t pr sous an thindayot at \n",
      "\n",
      "[28m 39s (17200 86%) 2.5213]\n",
      "Whe iollil ware y wisliowene hit meraren ink fowind f thathe, rith, here ne wind ind thein l tor Ca m, \n",
      "\n",
      "[28m 54s (17300 86%) 2.4891]\n",
      "Whe sel t hearid!\n",
      "SPEENI,\n",
      "\n",
      "Congat, toin the hrakillen s wevest then irushert hodorest wir y ss the, ho \n",
      "\n",
      "[29m 9s (17400 87%) 2.4886]\n",
      "Wh a che brenors at whes livee tist ma, RY:\n",
      "Whem that se t medstr an t t he sthist al, by Melwet warte \n",
      "\n",
      "[29m 24s (17500 87%) 2.4814]\n",
      "Whe f ow ake thald yo tnd macherlour mowne ardldere she s, w chard.\n",
      "ONCour that hachar we:\n",
      "A Serofes I \n",
      "\n",
      "[29m 42s (17600 88%) 2.4603]\n",
      "Wh whir s\n",
      "\n",
      "An nse,\n",
      "I thithemy t hin e.\n",
      "\n",
      "\n",
      "Thase thoule ou h comyo hiciove hit sin hatheased d hanganer  \n",
      "\n",
      "[29m 56s (17700 88%) 2.4634]\n",
      "Whathe thatodot honthesthe yordomy fe he ano th sther fo me t che sentu t melll d indd bepevorn bonchy \n",
      "\n",
      "[30m 12s (17800 89%) 2.5267]\n",
      "Whey s th, beit theane thevethe semave he ththee s nd r y f A at thand t onwayoulyicr thithu r ndeffou \n",
      "\n",
      "[30m 29s (17900 89%) 2.4338]\n",
      "Whe thel sthint u hene be boueed agn tor tonilthout pand ghay d bery, aceshineef than ferg be ondouar  \n",
      "\n",
      "[30m 45s (18000 90%) 2.4714]\n",
      "Wherdelde lf are n, marderve ano we wed y t ly frosu we r!\n",
      "Prechate.\n",
      "Awlyou bep cour arie by\n",
      "GO: I t w \n",
      "\n",
      "[31m 2s (18100 90%) 2.4243]\n",
      "Where I'sedent f tu  a ichin toup vel ge ace s bile igourisonge mes welloulil ar l sthe whider be mfat \n",
      "\n",
      "[31m 20s (18200 91%) 2.4781]\n",
      "Whoulthe st nd f MI:\n",
      "Br sh set ar d th.\n",
      "Wh henth inousor an the theas shererathe:\n",
      "F w ure tivest,\n",
      "Buri \n",
      "\n",
      "[31m 36s (18300 91%) 2.4699]\n",
      "Whetean\n",
      "Wan st:\n",
      "PUne?\n",
      "\n",
      "I ale whead w---th it wen ble. thye fowndwomat mo le.\n",
      "Mo d thalonthas lor s; t, \n",
      "\n",
      "[31m 52s (18400 92%) 2.4555]\n",
      "Wh, Ond chie s sper ofo it thak omen, qucargewenth ssees r d wilf bendisiberand lothe p moo lathio if  \n",
      "\n",
      "[32m 6s (18500 92%) 2.5027]\n",
      "Whe as tomeavanemes pe re d:\n",
      "Senvin the pouro ath nis bllese ared the nthee alice ul ar te.\n",
      "BA ang the \n",
      "\n",
      "[32m 20s (18600 93%) 2.5292]\n",
      "Whe at is is s sethin anth po haitareay than y ind himondof I he d thile anon hanflld g oyorouthipl an \n",
      "\n",
      "[32m 34s (18700 93%) 2.4906]\n",
      "Wharr, fe lly ld s h thetinor ayo she the oribeare u t t s sthrr s io f buthe s ousth.\n",
      "Bupes te t f fr \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32m 49s (18800 94%) 2.4995]\n",
      "Whand tho tl s my n, whan merd wh the n the thatry cee s ang seaveserer far wimithathe o ter s SAMas h \n",
      "\n",
      "[33m 3s (18900 94%) 2.5035]\n",
      "Whede be cks n baces s thourerd s?\n",
      "An be menour an ail ore avand, ll matowikeilis an.\n",
      "\n",
      "HIOKICE s s\n",
      "THe \n",
      "\n",
      "[33m 17s (19000 95%) 2.4507]\n",
      "Wheestay aithe astabllof city llouthe t beoure bun fe\n",
      "G al indice qupathes\n",
      "Yenofowe m,\n",
      "Tour ithicu, wn \n",
      "\n",
      "[33m 32s (19100 95%) 2.4431]\n",
      "Whamoocon whil II:\n",
      "I ppr; ars ir t cou bre seryom u t y nnd bot hand k cane thet a! ineaigofincrerset  \n",
      "\n",
      "[33m 40s (19200 96%) 2.5081]\n",
      "Wht He t hid'dit waprt llise t,\n",
      "\n",
      "CENIFrupothavesthy ke d he f then'lo he twht!\n",
      "Thy tand tloucke cend m \n",
      "\n",
      "[33m 47s (19300 96%) 2.5079]\n",
      "Why\n",
      "QUS:\n",
      "AS:\n",
      "DUKathtthan:\n",
      "\n",
      "S:\n",
      "CAfou bupoou RCUCINThis f at ba l hans, gr t I the!\n",
      "NGLIffanur hay tom?\n",
      " \n",
      "\n",
      "[33m 54s (19400 97%) 2.4966]\n",
      "Wherto u th tcertay! f ousatouges o y othanod tind aneder s\n",
      "I issth furr l.\n",
      "Yow sord s se lath her nth \n",
      "\n",
      "[34m 2s (19500 97%) 2.5067]\n",
      "Whe, ber wir somye I ayorerlin howiang, halindifofoudeave t I o t, il y tus tharath whe o he s y s lal \n",
      "\n",
      "[34m 9s (19600 98%) 2.4795]\n",
      "Wh tlis g pef n t hine hitheso t ois thige in f bes te hake be.\n",
      "\n",
      "TH:\n",
      "M:\n",
      "Thes bap thavo ngs ble.\n",
      "HA gee \n",
      "\n",
      "[34m 18s (19700 98%) 2.4660]\n",
      "Whestoury tlence coullamandse ty thero oy thand wiesearitheak atrter ad inomeat h\n",
      "\n",
      "ORYou f arene d the \n",
      "\n",
      "[34m 25s (19800 99%) 2.4480]\n",
      "Whee t lcr.\n",
      "My arorson wisof ame ppalilede ars t w!\n",
      "\n",
      "LINTher te\n",
      "THARINon t d the'sive, fowonove thof t \n",
      "\n",
      "[34m 33s (19900 99%) 2.5115]\n",
      "Whe hul he ma amatoorin pes be me lis h the; tharm I imy s fitorendied merariser hathelearghirend pell \n",
      "\n",
      "[34m 41s (20000 100%) 2.4929]\n",
      "Wher ceicat nst my ches the,\n",
      "\n",
      "\n",
      "TIO: ard fas k or Pile th witheer ndld ce and thicor te has\n",
      "BUCingres y \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 20000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7375635550>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYVNWZx/HvC7SAyKa0iuybGsQF\nbQHFFRAQXBKTjEs0jtGQjPuYyeASF9RR1JlookmMiUbNqNEMGhERV1BBARtk35dmX7rZm4Ze3/mj\nFnup21Xd9nY7v8/z9NNVt07d+/atql+dPvfULXN3RESkcWlS3wWIiEjNU7iLiDRCCncRkUZI4S4i\n0ggp3EVEGiGFu4hII6RwFxFphBTuIiKNkMJdRKQRalZfG+7QoYN37969vjYvIhJKc+bMyXH39GTt\n6i3cu3fvTmZmZn1tXkQklMxsXSrtNCwjItIIKdxFRBohhbuISCOkcBcRaYQU7iIijZDCXUSkEVK4\ni4g0QqEL9xXb9vHrD5aTk5tf36WIiDRYoQv3ldty+e0nq9i5v6C+SxERabBCF+4x+l5vEZFgoQt3\ns/quQESk4QtduMc46rqLiAQJXbir4y4iklzowj1GY+4iIsFCF+4acxcRSS504R6jnruISLCk4W5m\nLcxstpnNN7PFZjYuoN2/mNmSaJtXa77U+JZqb9UiIo1EKt/ElA8McfdcM0sDppvZe+4+M9bAzPoA\ndwGD3X2XmR1ZS/XGabaMiEiwpOHu7g7kRq+mRX/KJ+tPgd+5+67ofbbXZJGlxcbcNSwjIhIspTF3\nM2tqZvOA7cCH7j6rXJNjgWPNbIaZzTSzkTVdaLyW2lqxiEgjklK4u3uxu58CdAYGmFm/ck2aAX2A\n84ArgT+bWbvy6zGzMWaWaWaZ2dnZ365yEREJVKXZMu6+G5gGlO+ZbwTedvdCd18LLCcS9uXv/5y7\nZ7h7Rnp6erUKNs2FFBFJKpXZMumxXriZtQSGAcvKNfsHcH60TQciwzRrarbUsjTmLiISLJXZMh2B\nl8ysKZE3gzfcfZKZPQhkuvtE4H1guJktAYqBX7r7jtooWP12EZHkUpktswDon2D5faUuO3BH9KdO\naCqkiEiw0H1CVUPuIiLJhS7cYzTmLiISLHThrp67iEhyoQv3GHXcRUSChS7cTfNlRESSCl24x7gG\n3UVEAoUv3NVxFxFJKnzhHqV+u4hIsNCFuzruIiLJhS7cYzTkLiISLHThrrNCiogkF7pw/4a67iIi\nQUIX7rF+u4ZlRESChS/cNSojIpJU6MI9Rh13EZFgoQt3nX5ARCS50IV7jMbcRUSChS7cNeYuIpJc\n6MI9RicOExEJFrpwV8ddRCS50IV7jPrtIiLBwhfu6rqLiCQVvnCP0pC7iEiwpOFuZi3MbLaZzTez\nxWY2rpK2PzAzN7OMmi2z1DbUdRcRSapZCm3ygSHunmtmacB0M3vP3WeWbmRmrYFbgVm1UGcFrlF3\nEZFASXvuHpEbvZoW/UmUrA8BjwMHa668ijTPXUQkuZTG3M2sqZnNA7YDH7r7rHK39we6uPukWqgx\nMXXcRUQCpRTu7l7s7qcAnYEBZtYvdpuZNQGeBH6RbD1mNsbMMs0sMzs7u1oFq+MuIpJclWbLuPtu\nYBowstTi1kA/YJqZZQGDgImJDqq6+3PunuHuGenp6dUuGtRxFxGpTCqzZdLNrF30cktgGLAsdru7\n73H3Du7e3d27AzOBS9w9szYK1tfsiYgkl0rPvSMw1cwWAF8RGXOfZGYPmtkltVteMM1zFxEJlnQq\npLsvAPonWH5fQPvzvn1ZwdRxFxFJLryfUNWou4hIoNCFu74gW0QkufCFu4ZlRESSCl24x6jjLiIS\nLIThrq67iEgyIQz3CH3NnohIsNCFu8bcRUSSC124x6jfLiISLHThro67iEhyoQv3OHXdRUQChS7c\ndeIwEZHkQhfuMTr9gIhIsNCFu/rtIiLJhS7cYzTNXUQkWOjCXUPuIiLJhS7cY9RzFxEJFrpwN426\ni4gkFbpwj1HHXUQkWOjCXWPuIiLJhS7cY3RWSBGRYOEN9/ouQESkAQtduGtYRkQkudCFe4xGZURE\ngiUNdzNrYWazzWy+mS02s3EJ2txhZkvMbIGZfWxm3WqnXE2FFBFJRSo993xgiLufDJwCjDSzQeXa\nfA1kuPtJwP8Bj9dsmYmo6y4iEiRpuHtEbvRqWvTHy7WZ6u550aszgc41WmUpGnMXEUkupTF3M2tq\nZvOA7cCH7j6rkubXA+/VRHGV0Zi7iEiwlMLd3Yvd/RQiPfIBZtYvUTszuxrIAJ4IuH2MmWWaWWZ2\ndna1ClbPXUQkuSrNlnH33cA0YGT528xsGHAPcIm75wfc/zl3z3D3jPT09GqUW2pd3+reIiKNWyqz\nZdLNrF30cktgGLCsXJv+wB+JBPv22ig0vi3NlhERSapZCm06Ai+ZWVMibwZvuPskM3sQyHT3iUSG\nYQ4D/h79jtP17n5JbRUNGnMXEalM0nB39wVA/wTL7yt1eVgN1xVIY+4iIsmF9xOqGnUXEQkUunBX\nx11EJLnQhXuMxtxFRIKFLtw15i4iklzowj1GHXcRkWAhDHd13UVEkglhuEfoa/ZERIKFLtw15i4i\nklzowl1ERJILXbjHOu4alRERCRa+cNe4jIhIUqEL9xidfkBEJFjowl39dhGR5EIX7jEacxcRCRa6\ncNeQu4hIcqEL9xj13EVEgoUu3PU1eyIiyYUu3GPUcRcRCRa6cNeYu4hIcqEL9xidOExEJFhow11E\nRIKFNtzVbxcRCRa6cNeYu4hIcknD3cxamNlsM5tvZovNbFyCNs3N7HUzW2Vms8yse20UW4a67iIi\ngVLpuecDQ9z9ZOAUYKSZDSrX5npgl7v3Bp4EHqvZMr+hs0KKiCSXNNw9Ijd6NS36U77ffCnwUvTy\n/wFDrZZTWGeFFBEJltKYu5k1NbN5wHbgQ3efVa5JJ2ADgLsXAXuAI2qy0HgttbFSEZFGJqVwd/di\ndz8F6AwMMLN+5ZokytwKXWszG2NmmWaWmZ2dXfVqy9T0re4uItKoVWm2jLvvBqYBI8vdtBHoAmBm\nzYC2wM4E93/O3TPcPSM9Pb1aBccGe5TtIiLBUpktk25m7aKXWwLDgGXlmk0Ero1e/gHwidfSR0h1\n4jARkeSapdCmI/CSmTUl8mbwhrtPMrMHgUx3nwg8D/zVzFYR6bFfUWsVR2lYRkQkWNJwd/cFQP8E\ny+8rdfkg8MOaLS0xzYQUEUkudJ9QjdFUSBGRYKELd3XcRUSSC124x2jMXUQkWPjCXV13EZGkwhfu\nUeq4i4gEC124a567iEhyoQv3OA26i4gECl24a567iEhyoQv3GPXbRUSChS7c1XEXEUkudOEeoyF3\nEZFgoQt3fc2eiEhyoQv3mFo6o7CISKMQunBXv11EJLnQhXuM+u0iIsFCF+4achcRSS504R6jIXcR\nkWChC/fYuWWU7SIiwUIX7jqiKiKSXPjCPUpTIUVEgoUu3HVAVUQkudCFu4iIJBe6cFfHXUQkuaTh\nbmZdzGyqmS01s8VmdluCNm3N7B0zmx9tc13tlPsNDbmLiARrlkKbIuAX7j7XzFoDc8zsQ3dfUqrN\nTcASd7/YzNKB5Wb2irsX1HTBOnGYiEhySXvu7r7F3edGL+8DlgKdyjcDWlskeQ8DdhJ5U6g1rpnu\nIiKBUum5x5lZd6A/MKvcTc8AE4HNQGvgcncvqYH6KtZQGysVEWlkUj6gamaHAROA2919b7mbRwDz\ngGOAU4BnzKxNgnWMMbNMM8vMzs7+FmVrzF1EpDIphbuZpREJ9lfc/c0ETa4D3vSIVcBa4Pjyjdz9\nOXfPcPeM9PT0ahWsIXcRkeRSmS1jwPPAUnf/dUCz9cDQaPujgOOANTVVZCLquIuIBEtlzH0wcA2w\n0MzmRZfdDXQFcPdngYeAF81sIZFh8bHunlML9cZPHCYiIsGShru7TyfJcUx33wwMr6miUqExdxGR\nYOH7hKo67iIiSYUu3GM0z11EJFhow11ERIKFLtxjwzIacxcRCRa6cG8STfeSEqW7iEiQ8Ia7sl1E\nJFAIwz3yu0TjMiIigUIX7maGmb5DVUSkMqELd4gMzWhYRkQkWEjDXcMyIiKVCWW4m3ruIiKVCmW4\nN9GYu4hIpUIa7qZhGRGRSoQ43Ou7ChGRhiuU4W46oCoiUqlQhnsTM51bRkSkEiENdyjWuIyISKCQ\nhrsOqIqIVCac4d5EB1RFRCoTznDXPHcRkUqFNNw1LCMiUpnQhntxSX1XISLScIUy3Nu0TGPPgYL6\nLkNEpMFKGu5m1sXMpprZUjNbbGa3BbQ7z8zmRdt8WvOlfuOIVoewY7/CXUQkSLMU2hQBv3D3uWbW\nGphjZh+6+5JYAzNrB/weGOnu683syFqqF4C0pkZRscbcRUSCJO25u/sWd58bvbwPWAp0KtfsKuBN\nd18fbbe9pgstrWmTJhRpLqSISKAqjbmbWXegPzCr3E3HAu3NbJqZzTGzH9dMeYmlNTWKS3REVUQk\nSCrDMgCY2WHABOB2d9+bYD2nAUOBlsCXZjbT3VeUW8cYYAxA165dq1100yamnruISCVS6rmbWRqR\nYH/F3d9M0GQjMMXd97t7DvAZcHL5Ru7+nLtnuHtGenp6tYtu1kRj7iIilUlltowBzwNL3f3XAc3e\nBs42s2ZmdigwkMjYfK1o2qSJThwmIlKJVIZlBgPXAAvNbF502d1AVwB3f9bdl5rZFGABUAL82d0X\n1UbBEO25a8xdRCRQ0nB39+mApdDuCeCJmigqmWZNTT13EZFKhPITqmlNm3CwUD13EZEgoQz3Y9q1\nIDe/iFXbc+u7FBGRBimU4b4mez8AN74yp54rERFpmEIZ7k2bRA4B5OTq/DIiIomEMtxbpDUF4GBh\ncT1XIiLSMIUy3NOaRsrOK1C4i4gkEspwdzQNUkSkMqEM958M7hG/vO9gIe7Oim376rEiEZGGJZTh\nflSbFvz2yv4A3PuPRbz4RRbDn/yM2Wt31nNlIiINQyjDHeDoNi0A+Me8zYx7J/K9Iet27K/PkkRE\nGozQhnvHti0qLHtt9noWbdpTZtmcdTt546sNAHy0ZBsT529m1podlFTx9AU3vzqXce8sjl9/fMoy\nXv9qfTUqbzwWbNzN7ryK01EnLdjMwo17Etyj9qzfkVdmm4XFJSzdUv7M1HVjxbZ9rM0p29GYunw7\nn67IrpH1//XLLK7+c/mvVKgZ01fmMPKpz8gvqtpkhZzc/Fp/PezPL2L7voO1uo2YZVv3hr6zGNpw\nT2/dvMKyuet3c9HT03lxxlq63/kur3+1nu//4Uv+c8ICduTmc8PLmdz62tdc/txMBj/2Cdv2Bj9R\nikucPXmFAGzYmcekBVv4y4ys+O2/n7aasRMWsmzrXt74agNTl23n/cVb47fvySvkYGEx//LHL5m7\nfld8+QMTF3PtC7NrYA9844PFW6s8JLU6O5ec3PxIrQcKEwZhUXEJ2wP2kbtzyTMzOOXBD8u8CEY+\n9Rk3v/o1Fz8zPXDbz322mjsnLKiw/NVZ6/nz52vi15dt3UtBUeQ0E49PWcbb8zYFrvOcJ6aW2eb4\n95Zx4W8+5+15m5i1Zkd8efa+fJZtTT30P1qyLb5vvsraycqAYzt5BUX8buoq8gqKGP7kZ5z/39Pi\nt01euIXr/vIV174wm9z8Iq7600yycqofHPe+vZjpq3IS3jZ1+XaKiqt/ao5f/WMhy7buY+OuA0Dk\nTbOy9W3efYB1O/Zz4//OZeyEhdz06lwemVzxhLCPTl7KzFKPQ6o+X5kdv9/3fj+DAf/1MQBX/Wkm\nT7y/rEzbP3++himLtgCwbe9B3lu4JeXtXP/iV7w665s3p5FPfc65T0xj0+4DZdoVlzgT5myMdw7n\nbdjNuwu28EGp135DkfKXdTQ0sbnuiTwQHaYZO2FhfNnNr35dps2WPQcZ+MjHXDmgC6/N3sDAHofz\n8vUDSGvShNlZO/n9tNV8tiKbhQ8M5+zHp8bvt3FXHs98sip+feRTn5dZ77T/OI8/fb6GV2at5/zj\n0pm9didjXp7DyZ3b8vGyb7590N0pLnGaNa34/vqHaavZvu8g9198Arv2F/Cbj1fi7oy7tB97DhRy\n0ytz6XtMG3Jy87nxvF6M+Wvkk7qLx42gVfNm3PXmAl6bvYGFDwyndYs0AHrfPZnbh/Xh5iF9ABj6\nP59yWPNmLBo3gqv/PIuFm/aQNX50mToefW8Zz09fy4w7h9CpXcv48j0HCjl53Afx6+c+MY1nrz6N\nkf2OZtnWsuHn7vzh09V895ROHBNdxyOTIy9KM6NXeituOLsnG3bmcfdbkcfrYGExz0xdFT9/0IAe\nh8ffvM7odQRHtm7B8q37uPW1r1m+bR/jLzuxzDaLS5xXZq0D4La/RU5kuuLhCykoLmH4k5+yK6+Q\ne0Z9hxM7t6XdoWkcf3QbAL5YnYNhnNHriPi6bng5E4Cs8aP54bNfxi/n5hfx0DtLWLF9H78ccRyz\n1+7kqY9Wlul0rNi2j2OPal0mND5euo0vVu/gxy/M5s0bz2T22p0c0eoQ+ndtz/PT1/LjM7pR7M6d\nExYweeFWPrrjXHofeRhLt+zlqDYtuPvNb57T5b27YAs3vTqX449uTZsWaVza/xh27S/g5iF9WLJ5\nL72ObMWGnXl0bn8oANOWZzOwx+G0b3UIEHnj27w78mZeVOxs2n2Ac56YStuWaTx79Wnx/bI7r4DW\nLdJo2sQ4c/wnFWoAuHvUd+LLsvfl88fP1vB65gbm3TecVdtzmbNuJ1k78vjl8OOw6GkJzYw12bkc\n0ao5n67MZtf+Au6fGPlv+dahfVixLXK6ke53vht9vHZw0UnH0P2IVrRIa8LD7y6NPz5XPjeTNTn7\nOb17e/7+8zMr7Ku3523i85U5/PcPI1878fGy7Xy8bDt3v7WQt278pv3g8Z9w5YAuPHrZSQC8/GUW\n495Zwr6Dhfzr4B5893cz4m07t2/Jfww/jp7prViwcQ+ndz+cju1a0Lp5M8ySnnuxxpl7/UwrzMjI\n8MzMzG+1jtiDXJd6pbdidfa3/3ftslM78ebcTax4+EIOadaEg4XFjHtnMa/N3hBv89sr+/O/M9fF\ng+3pK/tzy2tl36RO6tyWBaWGIy7oexQfLtkGwBWnd+FvX23g9TGDuPy5mQA8d81pvDBjLTPXRNa5\n+pFR9Lp7cpl1XjmgKy3SmpT5T2XGnUP4cvUOPl2RzTvzNyf8m8rX1+2IQ1m3Iw+APkcexu3DjuU3\nH6+Iv0hjssaPrtJjeVn/Trz5dXAvvmVaUw5U4QNuF/Q9iueuOY0ed02O19r7yMO4/+ITGPRopKf4\nzs1nxf8zGH/ZiUxflcOkBRV7hr2PPKzGz3l0y5DePF2qQxGz+pFRvDJrHZ3bt+R/Z67nk2WJv7q4\nY9sWbNkT/F9q82ZN+Hzs+Zz56CeVfsPZ6kdGsb+giJMe+ICfnduTgT0O5ycvJn4NL394JMf9akqZ\nN2aAj+44l2G//jR+/c0bz+Sy338BwOgTO/Luwi1J601m0bgR9Lv//fj1rPGjycnNJ7+ohOJi56Uv\ns3h++tr4bXkFRfS97/2AtUWs+q8Lue31eWTvy4//PY9ediJ3VfJmGzPihKP44zUZANzwUiYfLd3G\nmkdG0aRJ9QLfzOa4e0bSdmEO93kbdvP1+l20TGvKnSns5Ibo5C7tGNXvaN7I3FAjbxph9O/DjuXJ\nj1Ykbyj1ql+nNizaVD/HMari5vN788zUim+G9enJy09m/oY9vPhFFgDv3XY23+nYplrr+qcI95hF\nm/Zw0dPBY7wiIg3JqzcM5MzeHap131TDPbQHVEvr16lt/PK9F/UtMz6cSHXfMUVEasLBKs5Gqo5G\nEe4AE/7tTD75xblcf1YPZtw5JLBd6+bNeOvGM7ni9C4AnH9c8i/q7tuxDS0rOYArFQ3scXiNreuQ\nZg3naXr/xX3ruwTevmlwfZdQbT8a2JWrB3VNuf2L150ePwtsbRtxwlF1sh2A/fkK95Sd1q09PdMP\ni1//1ejI0fqrBnbl1RsGcu9FfXns+yeycNwIWqQ1Zfz3TyJr/Gj+ct0AssaP5pYhveP3nXTLWbx7\n61kA3HHBsUy+7WwWPjCcHh1axdv89foBfH3vBbz200EVarlyQFduGxqdlXL8kWVue+vGM/n5ub3K\nLDuj5xHcPer4Mste/enApH/zDWf1SLj8h6d1jv/9iTx1+SkM6H44Sx4cwaJxIzgmwWcGYh669IT4\n5dKzaf7tvF68/JMBTLx5MHdccGyF+10xoEv88sAehzPzrqHx62f17kBGt/YJt3fCMZH/qoYefyRt\nWjSjw2GHMO6SSA2f/fJ8Lju1U5n2D3+3X5nrl/XvFH/jLu/7p3ausOzJy09O2DaRF/41g+sG9+DZ\nq0/jP0cel/L9Ytk0866hvH3TYC7PSFxf6Zk2WeNHM3Zk2edE1vjRZI0fzcld2rH20VEseGB4pdsd\n1PNwxpzTE6DS/2Zj6y3vozvO5W9jKj6/77uoL1njRzPl9rM5pUu7hOv8+8/PiF9+4OK+/PuwY+mZ\n3opxl5zAw989keUPj+SoNhWnM8fcOqQ3p3dvz1m9O7D6kVGsfmQUk245i9n3DOXh7/ZL+Pr41zO7\nB67vk1+cy43nffO6m/OrYUy+9ewybf54TQbXDOpW4b7v3XY2ndtH9t9D5Z5vAA9eegIr/+vCMste\n++kgTuzUlgdLvX5Ki001rU2NYsw9yLwNuzmxU9uU3vmnr8zh6udn8dClJ3DNGd0D2415OZMPlmyr\n8GIoKXFWbs9lxFOf8epPB3Jmrw6UlDhFJc6xv3ov3m7to6PIKyjm2hdmc1afDjz10Uou6HsUf/px\nBm99vZHe6a3ZmVfA2b07cPy9UygoLuGQZk3i870zfzWMjIc/YkCPw3njZ2eQm1/E1+t30aNDKz5c\nso3O7Q/lgr6RHkj5GSjf69+JdxdsYUW5J+L6HXmc80Rkuuesu4fy4KQl8Sltax8dFZ9FkjV+NLPW\n7OBAYTHn9EmPH+0vKCphxbZ9/D1zA6d2a8/EeZt5+qr+8RkIM+8aytFtW/DwpCUs3ryX18YMorjE\n6XX3ZM7oeQSvlQuQzKydHN+xDc2bNcE90nPfe7CQNtFpnRCZ7jh9VQ7nHhv5z2veht1c/+JXfHTH\nubRvdUj8b59991BKHI5q0xwzq7BPssaP5uUvs+I1T5izkf/+IHJw9+2bBnPp72bw9JX9Wbk9l9uG\n9ok/lw4UFPOd+6aUmR1zdp8OfL4yh58M7sHPz+vJ/W8v5rEfnMSBgmLyCorjnYPC4hL+MmMtizbt\nZWKpmUfz7x/OyeM+iE9RhchU0h53TWb0iR353Y9OpbzB4z9h0+4DLHtoJMffO6XMbfPvH07blpF9\ntvdgISc9EJm++qOBXXklOj1z+cMjad4s8l/p+4u30rp5M1q3SOOL1Tn8LNoJKSlx8otKeGTyUv46\ncx3v3noWJxwTGQo9WFjMbz9eyS1D+vCd+6Zw8cnHcOWALpzZq0N8Xyd644iJTduNGXNOTy4/vQu9\nSnXUgrw5dyP9OrXlnfmb2bTrAA9cegKtmzfj85U5nNS5Lfe8tYh3F27hB6d1jk95nDBnI7vyCrjh\n7Mib3hercrgq+oGwWJ3lnyPLHhrJ/vwiNuw6wCld2rE2Zz+78go4uk0LmphxdLRzNOo3n7Mk+pmI\n2LoKikrir//pY8/nrMcir7NnrurPRScdk/RvTCTVMXfcvV5+TjvtNG9o9h4oSNrmYGGRb9t7oErr\nLSgq9sKi4grrz9530E+4b4rPXbcz8L6rt+/zfQcLvdvYSd5t7CR3d9++96AfKChKut3tew/61j0H\nytw3Vf/++tf++JSl7u7Vur+7+/lPTPVuYyf51j1V21814WcvZ/qIJz+tsPzlL7M8M2uHP/TOYv9o\nydaE9z3z0Y+929hJvmt/fqXbeGf+Jt+254D/4A8z/A/TVrm7e2FRsZeUlKRc5/uLtni3sZP891Mj\n91+Xs9935Jbdbs6+g55fWJzw/tv2HPAZK7Pj284vLPZtew/42uzcCm0vfvpzP+uxj31KdJuZWTtS\nrrM6Hnpnsb8yc12lbQ4UFHlm1g4/UFDkO3Mr39/V8d7CLb4/v7DSNht27vcVW/fGry/dsscXbNjt\nE+ZsqNJr/dHJS73b2Em+cVdemeWx55N79V9LpQGZnkLGNuqee2NSXBJ5wBJ96CmZL1bngFPto/N/\nm72e4zu2CfwXPMi6Hft5Z/5mbjq/d718iKO61mTn8t6irdx0fu/kjb8ld2fn/gKOOCx4iKI27M4r\noN2hh9TpNhu74hJn696DFYbA9h4sJC+/mKPbtmBPXiHN05pU+iHMZP6ppkKKiPyzqLGpkGbWxcym\nmtlSM1tsZrdV0vZ0Mys2sx9UtWAREak5qZxbpgj4hbvPNbPWwBwz+9Ddl5RuZGZNgceAyj/HKyIi\ntS5pz93dt7j73OjlfcBSoFOCprcAE4DEJ7gQEZE6U6Wjc2bWHegPzCq3vBPwPeDZmipMRESqL+Vw\nN7PDiPTMb3f38mcPegoY6+6VfuzKzMaYWaaZZWZn18wXF4iISEUpzZYxszRgEvC+u/86we1rgdhc\ntw5AHjDG3f8RtE7NlhERqbpUZ8skPaBqkQnKzwNLEwU7gLv3KNX+RWBSZcEuIiK1K5XZMoOBa4CF\nZjYvuuxuoCuAu2ucXUSkgam3DzGZWTawrpp37wAk/hLJ+tVQ64KGW5vqqhrVVTWNsa5u7p70dLb1\nFu7fhpllpjLmVNcaal3QcGtTXVWjuqrmn7muRnPKXxER+YbCXUSkEQpruD9X3wUEaKh1QcOtTXVV\njeqqmn/aukI55i4iIpULa89dREQqEbpwN7ORZrbczFaZ2Z11vO2Epz82swfMbJOZzYv+jCp1n7ui\ntS43sxG1WFuWmS2Mbj8zuuxwM/vQzFZGf7ePLjcz+220rgVmVvH722qmpuNK7ZN5ZrbXzG6vj/1l\nZi+Y2XYzW1RqWZX3j5ldG22/0syuraW6njCzZdFtv2Vm7aLLu5vZgVL77dlS9zkt+vivitb+rb4d\nJaCuKj9uNf16Dajr9VI1ZcXLpnkqAAAEOElEQVQ+j1PH+ysoG+rvOZbK1zU1lB+gKbAa6AkcAswH\n+tbh9jsCp0YvtwZWAH2BB4D/SNC+b7TG5kCPaO1Na6m2LKBDuWWPA3dGL98JPBa9PAp4j8gpIwYB\ns+rosdsKdKuP/QWcA5wKLKru/gEOB9ZEf7ePXm5fC3UNB5pFLz9Wqq7upduVW89s4Ixoze8BF9ZC\nXVV63Grj9ZqornK3/w9wXz3sr6BsqLfnWNh67gOAVe6+xt0LgL8Bl9bVxj310x/HXAr8zd3z3X0t\nsIrI31BXLgVeil5+CfhuqeUve8RMoJ2ZdazlWoYCq929sg+u1dr+cvfPgJ0JtleV/TMC+NDdd7r7\nLuBDYGRN1+XuH7h7UfTqTKBzZeuI1tbG3b/0SEK8XOpvqbG6KhH0uNX467WyuqK9738BXqtsHbW0\nv4Kyod6eY2EL907AhlLXN1J5uNYaq3j645uj/169EPvXi7qt14EPzGyOmY2JLjvK3bdA5MkHHFkP\ndcVcQdkXXX3vL6j6/qmP/fYTIj28mB5m9rWZfWpmZ0eXdYrWUhd1VeVxq+v9dTawzd1XllpW5/ur\nXDbU23MsbOGeaFyszqf7WMXTH/8B6AWcAmwh8q8h1G29g939VOBC4CYzO6eStnW6H83sEOAS4O/R\nRQ1hf1UmqI663m/3EPkmtFeii7YAXd29P3AH8KqZtanDuqr6uNX143klZTsQdb6/EmRDYNOAGmqs\ntrCF+0agS6nrnYHNdVmARU5/PAF4xd3fBHD3be5e7O4lwJ/4Ziihzup1983R39uBt6I1bIsNt0R/\nx74lq67344XAXHffFq2x3vdXVFX3T53VFz2QdhHwo+jQAdFhjx3Ry3OIjGcfG62r9NBNrdRVjcet\nLvdXM+Ay4PVS9dbp/kqUDdTjcyxs4f4V0MfMekR7g1cAE+tq49ExvQqnPy43Xv09IHYkfyJwhZk1\nN7MeQB8iB3Jquq5WFvl+W8ysFZEDcoui248dbb8WeLtUXT+OHrEfBOyJ/etYS8r0qOp7f5VS1f3z\nPjDczNpHhySGUwvfGWxmI4GxwCXunldqebpFvqsYM+tJZP+sida2z8wGRZ+jPy71t9RkXVV93Ory\n9ToMWObu8eGWutxfQdlAfT7Hvs0R4vr4IXKUeQWRd+F76njbZxH5F2kBMC/6Mwr4K7Awunwi0LHU\nfe6J1rqcb3lEvpK6ehKZiTAfWBzbL8ARwMfAyujvw6PLDfhdtK6FQEYt7rNDgR1A21LL6nx/EXlz\n2QIUEukdXV+d/UNkDHxV9Oe6WqprFZFx19hz7Nlo2+9HH9/5wFzg4lLrySAStquBZ4h+QLGG66ry\n41bTr9dEdUWXvwj8vFzbutxfQdlQb88xfUJVRKQRCtuwjIiIpEDhLiLSCCncRUQaIYW7iEgjpHAX\nEWmEFO4iIo2Qwl1EpBFSuIuINEL/D04cLakj3YmNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7378bbf898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arij/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To mare:\n",
      "US:\n",
      "DUS: w d, ey omeris, f Bowind s\n",
      "Buchincear be, m'are.\n",
      "Limabeef d\n",
      "OPENTor ttheasst psh spoulline cl hed\n",
      "R:\n",
      "\n",
      "\n",
      "Wh we iloo'\n",
      "PThe beat.\n",
      "NThedor gaisowing me D sus tomyoou, athie t be arererounc\n",
      "----\n",
      "Thisu f d I t\n",
      "Brer bur w sis' hacor gut gomernou y t me'd'th brorend d 'sts m and Tourig se fan fe s le bobll lllalonthighencat he my I cus yofonsthesede the thameine f t! p, by wanoutht goreieroun de l\n",
      "----\n",
      "Thathere he thed gie inge t f wing t f t meme s f sthato gere or bo prerere thaper we s he the the meme d f sthee thar the tha the.\n",
      "\n",
      "Andou llo thame beng the the thef we fe sthas hing the gas oritheis t\n",
      "----\n",
      "The the he thes alle f t t the t the the the lllouthe he f and be at the win me or s t sthe g there the and s the theand s there ther the the the id che the tha the hes the the the wis in ghe the g meat\n",
      "----\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n"
     ]
    }
   ],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
